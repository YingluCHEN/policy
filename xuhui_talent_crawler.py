#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾æ±‡åŒºäººæ‰æ”¿ç­–çˆ¬è™«
é‡ç‚¹æå–ä¼ä¸šç”³æŠ¥è¦æ±‚
"""

import requests
import re
import json
import time
import pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import urljoin, urlparse
import os

class XuhuiTalentCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        
        # å¾æ±‡åŒºæ”¿åºœç½‘ç«™URLs - é‡ç‚¹å…³æ³¨äººæ‰æ”¿ç­–
        self.urls = [
            # æ”¿åŠ¡å…¬å¼€ - æ”¿ç­–æ–‡ä»¶
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_xxgyhb&page=1",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_xxgyhb&page=2",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_xxgyhb&page=3",
            # äººåŠ›èµ„æºå’Œç¤¾ä¼šä¿éšœå±€
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_rshbj&page=1",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_rshbj&page=2",
            # ç§‘å­¦æŠ€æœ¯å§”å‘˜ä¼š
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_kjw&page=1",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_kjw&page=2",
            # å•†åŠ¡å§”å‘˜ä¼š
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_sww&page=1",
            # å‘å±•å’Œæ”¹é©å§”å‘˜ä¼š
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_fgw&page=1",
            # å¾æ±‡åŒºæ”¿åºœä¸»ç«™æœç´¢äººæ‰ç›¸å…³
            "https://www.xuhui.gov.cn/search/pcRender?pageId=51bc01c2a0b04b5e8816d5eaea8c6df4&advance=true&tpl=2068&keyword=äººæ‰",
            "https://www.xuhui.gov.cn/search/pcRender?pageId=51bc01c2a0b04b5e8816d5eaea8c6df4&advance=true&tpl=2068&keyword=AI",
            "https://www.xuhui.gov.cn/search/pcRender?pageId=51bc01c2a0b04b5e8816d5eaea8c6df4&advance=true&tpl=2068&keyword=äººå·¥æ™ºèƒ½",
            "https://www.xuhui.gov.cn/search/pcRender?pageId=51bc01c2a0b04b5e8816d5eaea8c6df4&advance=true&tpl=2068&keyword=åˆ›ä¸š",
            "https://www.xuhui.gov.cn/search/pcRender?pageId=51bc01c2a0b04b5e8816d5eaea8c6df4&advance=true&tpl=2068&keyword=ç§‘æŠ€"
        ]
        
        # äººæ‰æ”¿ç­–å…³é”®è¯
        self.talent_keywords = [
            "äººæ‰", "é«˜å±‚æ¬¡", "é¢†å†›", "ä¸“å®¶", "åšå£«", "ç¡•å£«", "æµ·å¤–", "å¼•è¿›",
            "åˆ›ä¸š", "å­µåŒ–", "ç§‘æŠ€", "ç ”å‘", "åˆ›æ–°", "AI", "äººå·¥æ™ºèƒ½", "æ™ºèƒ½",
            "è¡¥è´´", "èµ„åŠ©", "å¥–åŠ±", "æ‰¶æŒ", "èµ„é‡‘", "æ”¯æŒ", "ä¸“é¡¹",
            "ç”³æŠ¥", "ç”³è¯·", "æ¡ä»¶", "è¦æ±‚", "ææ–™", "æˆªæ­¢", "æœŸé™"
        ]
        
        # ä¼ä¸šç”³æŠ¥è¦æ±‚å…³é”®è¯
        self.application_keywords = {
            "ä¼ä¸šè¦æ±‚": ["æ³¨å†Œåœ°", "æ³¨å†Œèµ„æœ¬", "è¥ä¸šæ”¶å…¥", "çº³ç¨", "è§„æ¨¡", "è¡Œä¸š", "èµ„è´¨", "æˆç«‹æ—¶é—´", "é«˜æ–°æŠ€æœ¯ä¼ä¸š", "ç‹¬è§’å…½", "ä»ä¸šäººå‘˜"],
            "ç”³æŠ¥æ¡ä»¶": ["ç”³æŠ¥æ¡ä»¶", "åŸºæœ¬æ¡ä»¶", "å‡†å…¥æ¡ä»¶", "èµ„æ ¼æ¡ä»¶", "å­¦å†è¦æ±‚", "å·¥ä½œç»éªŒ", "å¹´é¾„é™åˆ¶", "ä¸“ä¸šèƒŒæ™¯"],
            "ç”³æŠ¥ææ–™": ["ç”³æŠ¥ææ–™", "ç”³è¯·ææ–™", "è¯æ˜ææ–™", "é™„ä»¶", "ç”³è¯·è¡¨", "æ¨èä¿¡", "ç®€å†", "å­¦å†è¯æ˜", "å·¥ä½œè¯æ˜"],
            "ç”³æŠ¥æ—¶é—´": ["æˆªæ­¢æ—¶é—´", "ç”³æŠ¥æ—¶é—´", "ç”³æŠ¥æœŸé—´", "å—ç†æ—¶é—´", "åŠç†æ—¶é™", "æœ‰æ•ˆæœŸ", "è¯„å®¡æ—¶é—´", "å…¬ç¤ºæ—¶é—´"]
        }
        
        self.policies = []

    def fetch_page(self, url):
        """è·å–é¡µé¢å†…å®¹"""
        try:
            print(f"æ­£åœ¨çˆ¬å–: {url}")
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return response.text
        except Exception as e:
            print(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
            return None

    def extract_policy_links(self, html, base_url):
        """ä»åˆ—è¡¨é¡µæå–æ”¿ç­–é“¾æ¥"""
        if not html:
            return []
        
        soup = BeautifulSoup(html, 'html.parser')
        links = []
        
        # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æ”¿ç­–é“¾æ¥
        for a in soup.find_all('a', href=True):
            href = a.get('href')
            title = a.get_text(strip=True)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯äººæ‰ç›¸å…³æ”¿ç­–
            if any(keyword in title for keyword in self.talent_keywords):
                full_url = urljoin(base_url, href)
                links.append({
                    'title': title,
                    'url': full_url
                })
        
        return links

    def extract_policy_content(self, url):
        """æå–æ”¿ç­–è¯¦ç»†å†…å®¹"""
        html = self.fetch_page(url)
        if not html:
            return None
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # æå–æ ‡é¢˜
        title = ""
        title_selectors = ['h1', '.title', '.art-title', '[class*="title"]']
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title = title_elem.get_text(strip=True)
                break
        
        # æå–å†…å®¹
        content = ""
        content_selectors = ['.content', '.art-content', '[class*="content"]', '.main', 'article']
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                content = content_elem.get_text(strip=True)
                break
        
        # å¦‚æœæ²¡æ‰¾åˆ°ç‰¹å®šçš„å†…å®¹åŒºåŸŸï¼Œå–bodyå†…å®¹
        if not content:
            body = soup.find('body')
            if body:
                content = body.get_text(strip=True)
        
        # æå–å‘å¸ƒæ—¶é—´
        publish_date = ""
        date_patterns = [
            r'(\d{4}[-å¹´]\d{1,2}[-æœˆ]\d{1,2}[æ—¥]?)',
            r'(\d{4}/\d{1,2}/\d{1,2})',
            r'(\d{4}\.\d{1,2}\.\d{1,2})'
        ]
        for pattern in date_patterns:
            match = re.search(pattern, content + title)
            if match:
                publish_date = match.group(1)
                break
        
        # æå–å‘å¸ƒéƒ¨é—¨
        department = ""
        dept_keywords = ["å¾æ±‡åŒº", "ä¸Šæµ·å¸‚", "äººåŠ›èµ„æº", "ç§‘æŠ€", "å‘æ”¹", "å•†åŠ¡", "æˆ¿ç®¡", "æ•™è‚²"]
        for keyword in dept_keywords:
            if keyword in content or keyword in title:
                # å°è¯•æå–åŒ…å«å…³é”®è¯çš„éƒ¨é—¨åç§°
                dept_match = re.search(f'({keyword}[^ï¼Œã€‚ï¼›ï¼ï¼Ÿ\n]*?[å±€|å§”|åŠ|ä¸­å¿ƒ|éƒ¨])', content + title)
                if dept_match:
                    department = dept_match.group(1)
                    break
        
        return {
            'title': title,
            'url': url,
            'content': content[:2000],  # é™åˆ¶å†…å®¹é•¿åº¦
            'publish_date': publish_date,
            'department': department,
            'crawl_time': datetime.now().isoformat()
        }

    def classify_policy(self, policy):
        """æ”¿ç­–åˆ†ç±»"""
        text = f"{policy['title']} {policy['content']}"
        
        categories = {
            "äººæ‰å¼•è¿›": ["äººæ‰å¼•è¿›", "æµ·å¤–äººæ‰", "é«˜å±‚æ¬¡äººæ‰", "é¢†å†›äººæ‰", "ä¸“å®¶", "é™¢å£«", "åƒäººè®¡åˆ’"],
            "AI/äººå·¥æ™ºèƒ½": ["AI", "äººå·¥æ™ºèƒ½", "æ™ºèƒ½", "ç®—æ³•", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "å¤§æ¨¡å‹", "ç”Ÿæˆå¼AI"],
            "åˆ›ä¸šæ‰¶æŒ": ["åˆ›ä¸š", "å­µåŒ–", "åˆåˆ›", "åˆ›æ–°åˆ›ä¸š", "åŒåˆ›", "åˆ›å®¢", "ä¼—åˆ›ç©ºé—´"],
            "èµ„é‡‘è¡¥è´´": ["è¡¥è´´", "èµ„åŠ©", "å¥–åŠ±", "èµ„é‡‘æ”¯æŒ", "ä¸“é¡¹èµ„é‡‘", "æ‰¶æŒèµ„é‡‘"],
            "ä½æˆ¿ä¿éšœ": ["ä½æˆ¿", "ç§Ÿæˆ¿", "è´­æˆ¿", "å®‰å±…", "äººæ‰å…¬å¯“", "ä½æˆ¿è¡¥è´´"],
            "å­å¥³æ•™è‚²": ["å­å¥³", "æ•™è‚²", "å…¥å­¦", "å°±å­¦", "å­¦æ ¡", "æ•™è‚²ä¼˜æƒ "],
            "åŒ»ç–—ä¿éšœ": ["åŒ»ç–—", "å¥åº·", "å°±åŒ»", "ä½“æ£€", "åŒ»ç–—æœåŠ¡"],
            "è½æˆ·æœåŠ¡": ["è½æˆ·", "æˆ·ç±", "å±…ä½è¯", "ç§¯åˆ†", "è¿æˆ·"]
        }
        
        for category, keywords in categories.items():
            if any(keyword in text for keyword in keywords):
                return category
        
        return "å…¶ä»–"

    def extract_application_requirements(self, policy):
        """æå–ä¼ä¸šç”³æŠ¥è¦æ±‚"""
        text = f"{policy['title']} {policy['content']}"
        requirements = {}
        
        for req_type, keywords in self.application_keywords.items():
            found_requirements = []
            
            for keyword in keywords:
                # æŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„å¥å­
                sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', text)
                for sentence in sentences:
                    if keyword in sentence and len(sentence.strip()) > 10:
                        cleaned = sentence.strip()
                        if cleaned and len(cleaned) > 5:
                            found_requirements.append(cleaned)
            
            # å»é‡å¹¶é™åˆ¶æ•°é‡
            unique_requirements = list(dict.fromkeys(found_requirements))[:3]
            requirements[req_type] = "; ".join(unique_requirements) if unique_requirements else ""
        
        return requirements

    def crawl_all_policies(self):
        """çˆ¬å–æ‰€æœ‰æ”¿ç­–"""
        print("å¼€å§‹çˆ¬å–å¾æ±‡åŒºäººæ‰æ”¿ç­–...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs('data', exist_ok=True)
        
        all_links = []
        
        # ä»å„ä¸ªé¡µé¢æ”¶é›†æ”¿ç­–é“¾æ¥
        for url in self.urls:
            html = self.fetch_page(url)
            if html:
                links = self.extract_policy_links(html, url)
                all_links.extend(links)
            time.sleep(1)  # çˆ¬å–é—´éš”
        
        # å»é‡
        unique_links = []
        seen_urls = set()
        for link in all_links:
            if link['url'] not in seen_urls:
                unique_links.append(link)
                seen_urls.add(link['url'])
        
        print(f"æ‰¾åˆ° {len(unique_links)} ä¸ªäººæ‰ç›¸å…³æ”¿ç­–é“¾æ¥")
        
        # æå–æ¯ä¸ªæ”¿ç­–çš„è¯¦ç»†å†…å®¹
        for i, link in enumerate(unique_links[:20], 1):  # é™åˆ¶çˆ¬å–æ•°é‡
            print(f"æ­£åœ¨å¤„ç†ç¬¬ {i} ä¸ªæ”¿ç­–: {link['title'][:50]}...")
            
            policy = self.extract_policy_content(link['url'])
            if policy and len(policy['content']) > 100:  # è¿‡æ»¤æ‰å†…å®¹å¤ªå°‘çš„é¡µé¢
                policy['category'] = self.classify_policy(policy)
                policy['application_requirements'] = self.extract_application_requirements(policy)
                self.policies.append(policy)
            
            time.sleep(2)  # çˆ¬å–é—´éš”
        
        print(f"æˆåŠŸçˆ¬å– {len(self.policies)} ä¸ªæ”¿ç­–")

    def analyze_and_export(self):
        """åˆ†ææ•°æ®å¹¶å¯¼å‡º"""
        if not self.policies:
            print("æ²¡æœ‰çˆ¬å–åˆ°æœ‰æ•ˆæ•°æ®")
            return
        
        print("æ­£åœ¨åˆ†ææ•°æ®...")
        
        # ç»Ÿè®¡åˆ†æ
        categories = {}
        departments = {}
        policies_with_requirements = 0
        
        for policy in self.policies:
            # åˆ†ç±»ç»Ÿè®¡
            cat = policy.get('category', 'å…¶ä»–')
            categories[cat] = categories.get(cat, 0) + 1
            
            # éƒ¨é—¨ç»Ÿè®¡
            dept = policy.get('department', 'æœªçŸ¥')
            departments[dept] = departments.get(dept, 0) + 1
            
            # ç”³æŠ¥è¦æ±‚ç»Ÿè®¡
            req = policy.get('application_requirements', {})
            if any(v.strip() for v in req.values() if v):
                policies_with_requirements += 1
        
        # å¯¼å‡ºJSONæ ¼å¼
        output_data = {
            'crawl_time': datetime.now().isoformat(),
            'total_policies': len(self.policies),
            'statistics': {
                'categories': categories,
                'departments': departments,
                'policies_with_requirements': policies_with_requirements
            },
            'policies': self.policies
        }
        
        with open('data/talent_policies.json', 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        # å¯¼å‡ºCSVæ ¼å¼
        df_data = []
        for policy in self.policies:
            req = policy.get('application_requirements', {})
            df_data.append({
                'æ ‡é¢˜': policy.get('title', ''),
                'åˆ†ç±»': policy.get('category', ''),
                'å‘å¸ƒéƒ¨é—¨': policy.get('department', ''),
                'å‘å¸ƒæ—¶é—´': policy.get('publish_date', ''),
                'é“¾æ¥': policy.get('url', ''),
                'ä¼ä¸šè¦æ±‚': req.get('ä¼ä¸šè¦æ±‚', ''),
                'ç”³æŠ¥æ¡ä»¶': req.get('ç”³æŠ¥æ¡ä»¶', ''),
                'ç”³æŠ¥ææ–™': req.get('ç”³æŠ¥ææ–™', ''),
                'ç”³æŠ¥æ—¶é—´': req.get('ç”³æŠ¥æ—¶é—´', ''),
                'å†…å®¹æ‘˜è¦': policy.get('content', '')[:200]
            })
        
        df = pd.DataFrame(df_data)
        df.to_csv('data/talent_policies.csv', index=False, encoding='utf-8-sig')
        df.to_excel('data/talent_policies.xlsx', index=False)
        
        # å¯¼å‡ºä¼ä¸šç”³æŠ¥è¦æ±‚æ±‡æ€»
        with open('data/application_requirements.txt', 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write("å¾æ±‡åŒºäººæ‰æ”¿ç­–ä¼ä¸šç”³æŠ¥è¦æ±‚æ±‡æ€»\n")
            f.write("=" * 60 + "\n\n")
            
            for i, policy in enumerate(self.policies, 1):
                req = policy.get('application_requirements', {})
                has_req = any(v.strip() for v in req.values() if v)
                
                if has_req:
                    f.write(f"{i}. ã€{policy.get('category', 'å…¶ä»–')}ã€‘{policy.get('title', '')}\n")
                    f.write(f"   å‘å¸ƒéƒ¨é—¨: {policy.get('department', 'æœªçŸ¥')}\n")
                    f.write(f"   å‘å¸ƒæ—¶é—´: {policy.get('publish_date', 'æœªçŸ¥')}\n")
                    f.write(f"   é“¾æ¥: {policy.get('url', '')}\n")
                    
                    for req_type, req_content in req.items():
                        if req_content.strip():
                            f.write(f"   {req_type}: {req_content}\n")
                    f.write("\n" + "-" * 60 + "\n\n")
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        with open('data/analysis_report.txt', 'w', encoding='utf-8') as f:
            f.write("å¾æ±‡åŒºäººæ‰æ”¿ç­–åˆ†ææŠ¥å‘Š\n")
            f.write("=" * 40 + "\n\n")
            f.write(f"çˆ¬å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ€»æ”¿ç­–æ•°é‡: {len(self.policies)} æ¡\n")
            f.write(f"åŒ…å«ç”³æŠ¥è¦æ±‚çš„æ”¿ç­–: {policies_with_requirements} æ¡\n\n")
            
            f.write("æ”¿ç­–åˆ†ç±»åˆ†å¸ƒ:\n")
            for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
                percentage = count / len(self.policies) * 100
                f.write(f"  {cat}: {count}æ¡ ({percentage:.1f}%)\n")
            
            f.write(f"\nä¸»è¦å‘å¸ƒéƒ¨é—¨:\n")
            for dept, count in sorted(departments.items(), key=lambda x: x[1], reverse=True):
                f.write(f"  {dept}: {count}æ¡\n")
        
        print(f"\næ•°æ®å·²å¯¼å‡ºåˆ° data/ ç›®å½•:")
        print(f"- è¯¦ç»†æ•°æ®: data/talent_policies.json")
        print(f"- è¡¨æ ¼æ•°æ®: data/talent_policies.csv")
        print(f"- Excelæ–‡ä»¶: data/talent_policies.xlsx")
        print(f"- ç”³æŠ¥è¦æ±‚: data/application_requirements.txt")
        print(f"- åˆ†ææŠ¥å‘Š: data/analysis_report.txt")

def main():
    crawler = XuhuiTalentCrawler()
    crawler.crawl_all_policies()
    crawler.analyze_and_export()
    
    print("\nğŸ¯ çˆ¬å–å®Œæˆï¼é‡ç‚¹å…³æ³¨ä»¥ä¸‹æ–‡ä»¶:")
    print("ğŸ“‹ ä¼ä¸šç”³æŠ¥è¦æ±‚: data/application_requirements.txt")
    print("ğŸ“Š åˆ†ææŠ¥å‘Š: data/analysis_report.txt")

if __name__ == "__main__":
    main() 