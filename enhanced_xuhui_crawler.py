#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆå¾æ±‡åŒºäººæ‰æ”¿ç­–çˆ¬è™«
åŠ å¤§æ£€ç´¢åŠ›åº¦ï¼Œé‡ç‚¹æå–ä¼ä¸šç”³æŠ¥æ¡ä»¶
"""

import requests
import re
import json
import time
import pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import urljoin, urlparse
import os
from concurrent.futures import ThreadPoolExecutor, as_completed

class EnhancedXuhuiTalentCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        
        # å¤§å¹…æ‰©å±•URLè¦†ç›–èŒƒå›´
        self.urls = [
            # æ”¿åŠ¡å…¬å¼€ - æ‰©å±•åˆ°æ›´å¤šé¡µé¢
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_xxgyhb&page=1",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_xxgyhb&page=2", 
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_xxgyhb&page=3",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_xxgyhb&page=4",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_xxgyhb&page=5",
            
            # äººåŠ›èµ„æºå’Œç¤¾ä¼šä¿éšœå±€ - æ‰©å±•é¡µé¢
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_rshbj&page=1",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_rshbj&page=2",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_rshbj&page=3",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_rshbj&page=4",
            
            # ç§‘å­¦æŠ€æœ¯å§”å‘˜ä¼š - æ‰©å±•é¡µé¢
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_kjw&page=1",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_kjw&page=2",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_kjw&page=3",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_kjw&page=4",
            
            # å•†åŠ¡å§”å‘˜ä¼š
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_sww&page=1",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_sww&page=2",
            
            # å‘å±•å’Œæ”¹é©å§”å‘˜ä¼š
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_fgw&page=1",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_fgw&page=2",
            
            # æ•™è‚²å±€
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_jyj&page=1",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_jyj&page=2",
            
            # è´¢æ”¿å±€
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_czj&page=1",
            
            # å›½æœ‰èµ„äº§ç›‘ç£ç®¡ç†å§”å‘˜ä¼š
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_gzw&page=1",
            
            # é€šçŸ¥å…¬å‘Š
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_gggs&page=1",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_gggs&page=2",
            
            # è§„èŒƒæ€§æ–‡ä»¶
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_zcwj&page=1",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_zcwj&page=2",
            
            # å„å§”åŠå±€ä¸»ç«™
            "https://www.xuhui.gov.cn/renshebao/",
            "https://www.xuhui.gov.cn/kejiju/", 
            "https://www.xuhui.gov.cn/jiaoyu/",
            "https://www.xuhui.gov.cn/fangtuju/",
            
            # ä¸“é¢˜ä¸“æ 
            "https://www.xuhui.gov.cn/ztlm/",
            
            # é‡ç‚¹é¢†åŸŸä¿¡æ¯å…¬å¼€
            "https://www.xuhui.gov.cn/zdly/",
        ]
        
        # æ‰©å±•çš„äººæ‰æ”¿ç­–å…³é”®è¯
        self.talent_keywords = [
            # äººæ‰ç›¸å…³
            "äººæ‰", "é«˜å±‚æ¬¡äººæ‰", "é¢†å†›äººæ‰", "æ‹”å°–äººæ‰", "é’å¹´äººæ‰", "æµ·å¤–äººæ‰", "å¼•è¿›äººæ‰",
            "ä¸“å®¶", "åšå£«", "ç¡•å£«", "æ•™æˆ", "ç ”ç©¶å‘˜", "å·¥ç¨‹å¸ˆ", "ç§‘å­¦å®¶", "å­¦è€…",
            "åƒäººè®¡åˆ’", "ä¸‡äººè®¡åˆ’", "æ°é’", "é•¿æ±Ÿå­¦è€…", "é™¢å£«", "åšå£«å", "æµ·å½’", "ç•™å­¦äººå‘˜",
            
            # AIå’Œç§‘æŠ€ç›¸å…³  
            "AI", "äººå·¥æ™ºèƒ½", "æ™ºèƒ½", "ç®—æ³•", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "å¤§æ¨¡å‹", "ç”Ÿæˆå¼AI",
            "å…·èº«æ™ºèƒ½", "å¤§æ¨¡å‹", "ç§‘åˆ›", "ç§‘æŠ€", "ç ”å‘", "åˆ›æ–°", "æŠ€æœ¯", "æ•°å­—åŒ–",
            "æ–°å‹å·¥ä¸šåŒ–", "å…ˆå¯¼åŒº", "ç§‘åˆ›è¡—åŒº", "å­µåŒ–å™¨", "åŠ é€Ÿå™¨",
            
            # åˆ›ä¸šæ”¯æŒ
            "åˆ›ä¸š", "å­µåŒ–", "åˆåˆ›", "åˆ›æ–°åˆ›ä¸š", "åŒåˆ›", "åˆ›å®¢", "ä¼—åˆ›ç©ºé—´", "ç§‘æŠ€å›­",
            "äº§ä¸šå›­", "å¤©ä½¿æŠ•èµ„", "é£æŠ•", "èèµ„", "æŠ•èµ„",
            
            # èµ„é‡‘æ”¯æŒ
            "è¡¥è´´", "èµ„åŠ©", "å¥–åŠ±", "æ‰¶æŒ", "èµ„é‡‘", "æ”¯æŒ", "ä¸“é¡¹", "åŸºé‡‘", "ç»è´¹",
            "è´¢æ”¿", "æ‹¨æ¬¾", "è¡¥åŠ©", "è´´æ¯", "å‡å…", "ä¼˜æƒ ",
            
            # ç”³æŠ¥ç›¸å…³
            "ç”³æŠ¥", "ç”³è¯·", "æ¡ä»¶", "è¦æ±‚", "ææ–™", "æˆªæ­¢", "æœŸé™", "è¯„å®¡", "å®¡æ ¸",
            "å‡†å…¥", "èµ„æ ¼", "æ ‡å‡†", "é—¨æ§›", "æµç¨‹", "ç¨‹åº"
        ]
        
        # å¤§å¹…å¢å¼ºçš„ä¼ä¸šç”³æŠ¥æ¡ä»¶å…³é”®è¯
        self.application_keywords = {
            "ä¼ä¸šåŸºæœ¬è¦æ±‚": [
                "æ³¨å†Œåœ°", "æ³¨å†Œèµ„æœ¬", "å®ç¼´èµ„æœ¬", "è¥ä¸šæ”¶å…¥", "å¹´æ”¶å…¥", "å¹´è¥ä¸šé¢", 
                "çº³ç¨", "ç¨æ”¶", "è§„æ¨¡", "è¡Œä¸š", "èµ„è´¨", "æˆç«‹æ—¶é—´", "ç»è¥æœŸé™",
                "é«˜æ–°æŠ€æœ¯ä¼ä¸š", "ç‹¬è§’å…½ä¼ä¸š", "ä¸“ç²¾ç‰¹æ–°", "å°å·¨äºº", "çªç¾šä¼ä¸š",
                "ä»ä¸šäººå‘˜", "å‘˜å·¥æ•°", "æŠ€æœ¯äººå‘˜", "ç ”å‘äººå‘˜", "åšå£«æ•°é‡", "ç¡•å£«æ•°é‡"
            ],
            "ç”³æŠ¥æ¡ä»¶": [
                "ç”³æŠ¥æ¡ä»¶", "åŸºæœ¬æ¡ä»¶", "å‡†å…¥æ¡ä»¶", "èµ„æ ¼æ¡ä»¶", "ç”³è¯·æ¡ä»¶",
                "å­¦å†è¦æ±‚", "å·¥ä½œç»éªŒ", "å¹´é¾„é™åˆ¶", "ä¸“ä¸šèƒŒæ™¯", "æŠ€æœ¯è¦æ±‚",
                "ä¸šç»©è¦æ±‚", "è´¢åŠ¡è¦æ±‚", "ä¿¡ç”¨è¦æ±‚", "åˆè§„è¦æ±‚", "ç¯ä¿è¦æ±‚"
            ],
            "ç”³æŠ¥ææ–™": [
                "ç”³æŠ¥ææ–™", "ç”³è¯·ææ–™", "è¯æ˜ææ–™", "é™„ä»¶", "ç”³è¯·è¡¨", "ç”³æŠ¥è¡¨",
                "æ¨èä¿¡", "ç®€å†", "å­¦å†è¯æ˜", "å·¥ä½œè¯æ˜", "ä¸šç»©è¯æ˜",
                "è´¢åŠ¡æŠ¥è¡¨", "å®¡è®¡æŠ¥å‘Š", "ç¨åŠ¡è¯æ˜", "èµ„è´¨è¯ä¹¦", "ä¸“åˆ©è¯ä¹¦"
            ],
            "ç”³æŠ¥æ—¶é—´": [
                "æˆªæ­¢æ—¶é—´", "ç”³æŠ¥æ—¶é—´", "ç”³æŠ¥æœŸé—´", "å—ç†æ—¶é—´", "åŠç†æ—¶é™",
                "æœ‰æ•ˆæœŸ", "è¯„å®¡æ—¶é—´", "å…¬ç¤ºæ—¶é—´", "å‘å¸ƒæ—¶é—´", "å¼€å§‹æ—¶é—´",
                "ç»“æŸæ—¶é—´", "å¹´åº¦ç”³æŠ¥", "å­£åº¦ç”³æŠ¥", "æœˆåº¦ç”³æŠ¥"
            ],
            "èµ„é‡‘æ”¯æŒ": [
                "èµ„é‡‘æ”¯æŒ", "è¡¥è´´æ ‡å‡†", "å¥–åŠ±æ ‡å‡†", "æ”¯æŒé¢åº¦", "æœ€é«˜æ”¯æŒ",
                "æŒ‰æ¯”ä¾‹", "ä¸€æ¬¡æ€§", "åˆ†æœŸ", "å¹´åº¦æ”¯æŒ", "é…å¥—èµ„é‡‘",
                "ä¸“é¡¹èµ„é‡‘", "å¼•å¯¼åŸºé‡‘", "é£é™©è¡¥å¿", "è´´æ¯æ”¯æŒ"
            ],
            "å…¶ä»–ä¼˜æƒ ": [
                "ç¨æ”¶ä¼˜æƒ ", "ç§Ÿé‡‘ä¼˜æƒ ", "ç”¨åœ°ä¼˜æƒ ", "äººæ‰æœåŠ¡", "ç»¿è‰²é€šé“",
                "ä¼˜å…ˆæ¨è", "é‡ç‚¹æ”¯æŒ", "é…å¥—æœåŠ¡", "æ”¿ç­–å€¾æ–œ"
            ]
        }
        
        self.policies = []

    def fetch_page(self, url):
        """è·å–é¡µé¢å†…å®¹"""
        try:
            print(f"æ­£åœ¨çˆ¬å–: {url}")
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return response.text
        except Exception as e:
            print(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
            return None

    def extract_policy_links(self, html, base_url):
        """ä»åˆ—è¡¨é¡µæå–æ”¿ç­–é“¾æ¥ - å¢å¼ºç‰ˆ"""
        if not html:
            return []
        
        soup = BeautifulSoup(html, 'html.parser')
        links = []
        
        # å¤šç§é€‰æ‹©å™¨ç­–ç•¥
        link_selectors = [
            'a[href*="detail"]',  # åŒ…å«detailçš„é“¾æ¥
            'a[href*="article"]', # åŒ…å«articleçš„é“¾æ¥
            '.list-item a',       # åˆ—è¡¨é¡¹ä¸­çš„é“¾æ¥
            '.title a',           # æ ‡é¢˜é“¾æ¥
            'li a',               # åˆ—è¡¨ä¸­çš„é“¾æ¥
        ]
        
        found_links = []
        for selector in link_selectors:
            found_links.extend(soup.select(selector))
        
        # å»é‡å¹¶è¿‡æ»¤
        seen_hrefs = set()
        for a in found_links:
            href = a.get('href')
            title = a.get_text(strip=True)
            
            if href and href not in seen_hrefs and len(title) > 5:
                # æ›´å®½æ¾çš„äººæ‰æ”¿ç­–è¯†åˆ«
                is_talent_related = (
                    any(keyword in title for keyword in self.talent_keywords) or
                    any(keyword in title.lower() for keyword in ['policy', 'support', 'fund', 'grant']) or
                    len([k for k in self.talent_keywords if k in title]) > 0
                )
                
                if is_talent_related:
                    full_url = urljoin(base_url, href)
                    links.append({
                        'title': title,
                        'url': full_url
                    })
                    seen_hrefs.add(href)
        
        return links

    def extract_policy_content(self, url):
        """æå–æ”¿ç­–è¯¦ç»†å†…å®¹ - å¢å¼ºç‰ˆ"""
        html = self.fetch_page(url)
        if not html:
            return None
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # æå–æ ‡é¢˜ - å¤šç§ç­–ç•¥
        title = ""
        title_selectors = [
            'h1', 'h2', '.title', '.art-title', '.page-title', 
            '[class*="title"]', '[class*="head"]', '.main-title'
        ]
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem and len(title_elem.get_text(strip=True)) > 5:
                title = title_elem.get_text(strip=True)
                break
        
        # æå–å†…å®¹ - å¤šç§ç­–ç•¥
        content = ""
        content_selectors = [
            '.content', '.art-content', '.main-content', '.policy-content',
            '[class*="content"]', '.main', 'article', '.detail', '.text'
        ]
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # ç§»é™¤å¯¼èˆªã€èœå•ç­‰æ— å…³å†…å®¹
                for nav in content_elem.select('nav, .nav, .menu, .breadcrumb, .pagination'):
                    nav.decompose()
                content = content_elem.get_text(strip=True)
                if len(content) > 200:  # ç¡®ä¿å†…å®¹è¶³å¤Ÿé•¿
                    break
        
        # å¦‚æœæ²¡æ‰¾åˆ°ç‰¹å®šçš„å†…å®¹åŒºåŸŸï¼Œå–bodyå†…å®¹ä½†æ’é™¤å¯¼èˆª
        if not content or len(content) < 100:
            body = soup.find('body')
            if body:
                # ç§»é™¤å¯¼èˆªå…ƒç´ 
                for elem in body.select('nav, .nav, .menu, .sidebar, .footer, .header'):
                    elem.decompose()
                content = body.get_text(strip=True)
        
        # æå–å‘å¸ƒæ—¶é—´ - å¢å¼ºæ¨¡å¼
        publish_date = ""
        date_patterns = [
            r'(\d{4}[-å¹´]\d{1,2}[-æœˆ]\d{1,2}[æ—¥]?)',
            r'(\d{4}/\d{1,2}/\d{1,2})',
            r'(\d{4}\.\d{1,2}\.\d{1,2})',
            r'å‘å¸ƒæ—¶é—´[ï¼š:]\s*(\d{4}[-å¹´]\d{1,2}[-æœˆ]\d{1,2})',
            r'æ—¶é—´[ï¼š:]\s*(\d{4}[-å¹´]\d{1,2}[-æœˆ]\d{1,2})'
        ]
        
        search_text = content + title + str(soup)
        for pattern in date_patterns:
            match = re.search(pattern, search_text)
            if match:
                publish_date = match.group(1)
                break
        
        # æå–å‘å¸ƒéƒ¨é—¨ - å¢å¼ºæ¨¡å¼
        department = ""
        dept_patterns = [
            r'å‘å¸ƒæœºæ„[ï¼š:]\s*([^ï¼Œã€‚ï¼›ï¼ï¼Ÿ\n]+)',
            r'å‘å¸ƒéƒ¨é—¨[ï¼š:]\s*([^ï¼Œã€‚ï¼›ï¼ï¼Ÿ\n]+)',
            r'(å¾æ±‡åŒº[^ï¼Œã€‚ï¼›ï¼ï¼Ÿ\n]*?[å±€|å§”|åŠ|ä¸­å¿ƒ|éƒ¨|å¤„])',
            r'(ä¸Šæµ·å¸‚å¾æ±‡åŒº[^ï¼Œã€‚ï¼›ï¼ï¼Ÿ\n]*?[å±€|å§”|åŠ|ä¸­å¿ƒ|éƒ¨|å¤„])'
        ]
        
        for pattern in dept_patterns:
            match = re.search(pattern, search_text)
            if match:
                department = match.group(1).strip()
                break
        
        return {
            'title': title,
            'url': url,
            'content': content[:5000],  # å¢åŠ å†…å®¹é•¿åº¦
            'publish_date': publish_date,
            'department': department,
            'crawl_time': datetime.now().isoformat()
        }

    def classify_policy(self, policy):
        """æ”¿ç­–åˆ†ç±» - å¢å¼ºç‰ˆ"""
        text = f"{policy['title']} {policy['content']}"
        
        # æ›´ç²¾ç¡®çš„åˆ†ç±»è§„åˆ™
        categories = {
            "AI/äººå·¥æ™ºèƒ½": [
                "AI", "äººå·¥æ™ºèƒ½", "æ™ºèƒ½", "ç®—æ³•", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", 
                "å¤§æ¨¡å‹", "ç”Ÿæˆå¼AI", "å…·èº«æ™ºèƒ½", "å¤§æ¨¡å‹", "å…ˆå¯¼åŒº"
            ],
            "äººæ‰å¼•è¿›": [
                "äººæ‰å¼•è¿›", "æµ·å¤–äººæ‰", "é«˜å±‚æ¬¡äººæ‰", "é¢†å†›äººæ‰", "ä¸“å®¶å¼•è¿›",
                "é™¢å£«", "åƒäººè®¡åˆ’", "ä¸‡äººè®¡åˆ’", "æ°é’", "é•¿æ±Ÿå­¦è€…"
            ],
            "åˆ›ä¸šæ‰¶æŒ": [
                "åˆ›ä¸š", "å­µåŒ–", "åˆåˆ›", "åˆ›æ–°åˆ›ä¸š", "åŒåˆ›", "åˆ›å®¢", 
                "ä¼—åˆ›ç©ºé—´", "å­µåŒ–å™¨", "åŠ é€Ÿå™¨"
            ],
            "èµ„é‡‘è¡¥è´´": [
                "è¡¥è´´", "èµ„åŠ©", "å¥–åŠ±", "èµ„é‡‘æ”¯æŒ", "ä¸“é¡¹èµ„é‡‘", "æ‰¶æŒèµ„é‡‘",
                "è´¢æ”¿è¡¥è´´", "ç§‘ç ”ç»è´¹", "å¯åŠ¨èµ„é‡‘"
            ],
            "äº§ä¸šå‘å±•": [
                "äº§ä¸šå‘å±•", "æ–°å‹å·¥ä¸šåŒ–", "ç§‘æŠ€å›­", "äº§ä¸šå›­", "ç§‘åˆ›è¡—åŒº",
                "ä½ç©ºç»æµ", "å…ƒå®‡å®™", "æ•°å­—åŒ–"
            ],
            "ä½æˆ¿ä¿éšœ": [
                "ä½æˆ¿", "ç§Ÿæˆ¿", "è´­æˆ¿", "å®‰å±…", "äººæ‰å…¬å¯“", "ä½æˆ¿è¡¥è´´"
            ],
            "å­å¥³æ•™è‚²": [
                "å­å¥³", "æ•™è‚²", "å…¥å­¦", "å°±å­¦", "å­¦æ ¡", "æ•™è‚²ä¼˜æƒ "
            ],
            "è½æˆ·æœåŠ¡": [
                "è½æˆ·", "æˆ·ç±", "å±…ä½è¯", "ç§¯åˆ†", "è¿æˆ·"
            ]
        }
        
        # è®¡ç®—æ¯ä¸ªåˆ†ç±»çš„å¾—åˆ†
        category_scores = {}
        for category, keywords in categories.items():
            score = 0
            for keyword in keywords:
                # æ ‡é¢˜ä¸­çš„å…³é”®è¯æƒé‡æ›´é«˜
                if keyword in policy['title']:
                    score += 3
                # å†…å®¹ä¸­çš„å…³é”®è¯
                if keyword in policy['content']:
                    score += 1
            category_scores[category] = score
        
        # è¿”å›å¾—åˆ†æœ€é«˜çš„åˆ†ç±»
        if category_scores:
            best_category = max(category_scores, key=category_scores.get)
            if category_scores[best_category] > 0:
                return best_category
        
        return "å…¶ä»–"

    def extract_application_requirements(self, policy):
        """æå–ä¼ä¸šç”³æŠ¥è¦æ±‚ - å¤§å¹…å¢å¼ºç‰ˆ"""
        text = f"{policy['title']} {policy['content']}"
        requirements = {}
        
        for req_type, keywords in self.application_keywords.items():
            found_requirements = []
            
            for keyword in keywords:
                # ä½¿ç”¨æ›´ç²¾ç¡®çš„å¥å­åˆ†å‰²
                sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›\n]', text)
                for sentence in sentences:
                    if keyword in sentence and len(sentence.strip()) > 10:
                        cleaned = sentence.strip()
                        # è¿‡æ»¤æ‰å¤ªçŸ­æˆ–æ— æ„ä¹‰çš„å¥å­
                        if (len(cleaned) > 15 and 
                            not cleaned.startswith(('é¦–é¡µ', 'è¿”å›', 'ä¸Šä¸€é¡µ', 'ä¸‹ä¸€é¡µ')) and
                            not all(c in '=- \t' for c in cleaned)):
                            found_requirements.append(cleaned)
            
            # å»é‡ã€æ’åºå¹¶é™åˆ¶æ•°é‡
            unique_requirements = []
            seen = set()
            for req in found_requirements:
                if req not in seen and len(req) < 300:  # é¿å…è¿‡é•¿çš„å¥å­
                    unique_requirements.append(req)
                    seen.add(req)
                if len(unique_requirements) >= 5:  # å¢åŠ æ¯ç±»çš„æ•°é‡
                    break
            
            requirements[req_type] = "; ".join(unique_requirements) if unique_requirements else ""
        
        return requirements

    def crawl_all_policies(self):
        """çˆ¬å–æ‰€æœ‰æ”¿ç­– - å¢å¼ºç‰ˆ"""
        print("å¼€å§‹å¤§åŠ›åº¦çˆ¬å–å¾æ±‡åŒºäººæ‰æ”¿ç­–...")
        print(f"å°†çˆ¬å– {len(self.urls)} ä¸ªURLæº")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs('data', exist_ok=True)
        
        all_links = []
        
        # å¹¶å‘çˆ¬å–é“¾æ¥
        def fetch_links(url):
            html = self.fetch_page(url)
            if html:
                return self.extract_policy_links(html, url)
            return []
        
        # ä½¿ç”¨çº¿ç¨‹æ± åŠ é€Ÿé“¾æ¥æ”¶é›†
        with ThreadPoolExecutor(max_workers=3) as executor:
            future_to_url = {executor.submit(fetch_links, url): url for url in self.urls}
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    links = future.result()
                    all_links.extend(links)
                    print(f"ä» {url} è·å–åˆ° {len(links)} ä¸ªç›¸å…³é“¾æ¥")
                except Exception as e:
                    print(f"å¤„ç† {url} æ—¶å‡ºé”™: {e}")
                time.sleep(0.5)  # æ§åˆ¶é¢‘ç‡
        
        # å»é‡
        unique_links = []
        seen_urls = set()
        for link in all_links:
            if link['url'] not in seen_urls:
                unique_links.append(link)
                seen_urls.add(link['url'])
        
        print(f"å»é‡åå…±æ‰¾åˆ° {len(unique_links)} ä¸ªäººæ‰ç›¸å…³æ”¿ç­–é“¾æ¥")
        
        # é™åˆ¶çˆ¬å–æ•°é‡ä½†å¢åŠ åˆ°50ä¸ª
        target_links = unique_links[:50]
        print(f"å°†è¯¦ç»†çˆ¬å–å‰ {len(target_links)} ä¸ªæ”¿ç­–")
        
        # æå–æ¯ä¸ªæ”¿ç­–çš„è¯¦ç»†å†…å®¹
        for i, link in enumerate(target_links, 1):
            print(f"æ­£åœ¨å¤„ç†ç¬¬ {i}/{len(target_links)} ä¸ªæ”¿ç­–: {link['title'][:60]}...")
            
            policy = self.extract_policy_content(link['url'])
            if policy and len(policy['content']) > 200:  # æé«˜å†…å®¹è´¨é‡è¦æ±‚
                policy['category'] = self.classify_policy(policy)
                policy['application_requirements'] = self.extract_application_requirements(policy)
                
                # åªä¿ç•™æœ‰å®é™…ç”³æŠ¥è¦æ±‚çš„æ”¿ç­–
                req = policy['application_requirements']
                has_meaningful_req = any(
                    len(v.strip()) > 20 for v in req.values() if v
                )
                
                if has_meaningful_req or policy['category'] in ['AI/äººå·¥æ™ºèƒ½', 'äººæ‰å¼•è¿›', 'åˆ›ä¸šæ‰¶æŒ']:
                    self.policies.append(policy)
                    print(f"  âœ… å·²ä¿å­˜ (åˆ†ç±»: {policy['category']})")
                else:
                    print(f"  âŒ è·³è¿‡ (æ— æœ‰æ•ˆç”³æŠ¥è¦æ±‚)")
            
            time.sleep(1.5)  # çˆ¬å–é—´éš”
        
        print(f"\nğŸ¯ æˆåŠŸçˆ¬å– {len(self.policies)} ä¸ªé«˜è´¨é‡æ”¿ç­–")

    def analyze_and_export(self):
        """åˆ†ææ•°æ®å¹¶å¯¼å‡º - å¢å¼ºç‰ˆ"""
        if not self.policies:
            print("æ²¡æœ‰çˆ¬å–åˆ°æœ‰æ•ˆæ•°æ®")
            return
        
        print("æ­£åœ¨æ·±åº¦åˆ†ææ•°æ®...")
        
        # ç»Ÿè®¡åˆ†æ
        categories = {}
        departments = {}
        policies_with_requirements = 0
        ai_policies = 0
        
        for policy in self.policies:
            # åˆ†ç±»ç»Ÿè®¡
            cat = policy.get('category', 'å…¶ä»–')
            categories[cat] = categories.get(cat, 0) + 1
            
            if cat == 'AI/äººå·¥æ™ºèƒ½':
                ai_policies += 1
            
            # éƒ¨é—¨ç»Ÿè®¡
            dept = policy.get('department', 'æœªçŸ¥')
            departments[dept] = departments.get(dept, 0) + 1
            
            # ç”³æŠ¥è¦æ±‚ç»Ÿè®¡
            req = policy.get('application_requirements', {})
            if any(v.strip() for v in req.values() if v):
                policies_with_requirements += 1
        
        # å¯¼å‡ºJSONæ ¼å¼
        output_data = {
            'crawl_time': datetime.now().isoformat(),
            'total_policies': len(self.policies),
            'ai_policies': ai_policies,
            'statistics': {
                'categories': categories,
                'departments': departments,
                'policies_with_requirements': policies_with_requirements
            },
            'policies': self.policies
        }
        
        with open('data/enhanced_talent_policies.json', 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        # å¯¼å‡ºè¯¦ç»†çš„CSVæ ¼å¼
        df_data = []
        for policy in self.policies:
            req = policy.get('application_requirements', {})
            df_data.append({
                'æ ‡é¢˜': policy.get('title', ''),
                'åˆ†ç±»': policy.get('category', ''),
                'å‘å¸ƒéƒ¨é—¨': policy.get('department', ''),
                'å‘å¸ƒæ—¶é—´': policy.get('publish_date', ''),
                'é“¾æ¥': policy.get('url', ''),
                'ä¼ä¸šåŸºæœ¬è¦æ±‚': req.get('ä¼ä¸šåŸºæœ¬è¦æ±‚', ''),
                'ç”³æŠ¥æ¡ä»¶': req.get('ç”³æŠ¥æ¡ä»¶', ''),
                'ç”³æŠ¥ææ–™': req.get('ç”³æŠ¥ææ–™', ''),
                'ç”³æŠ¥æ—¶é—´': req.get('ç”³æŠ¥æ—¶é—´', ''),
                'èµ„é‡‘æ”¯æŒ': req.get('èµ„é‡‘æ”¯æŒ', ''),
                'å…¶ä»–ä¼˜æƒ ': req.get('å…¶ä»–ä¼˜æƒ ', ''),
                'å†…å®¹æ‘˜è¦': policy.get('content', '')[:300]
            })
        
        df = pd.DataFrame(df_data)
        df.to_csv('data/enhanced_talent_policies.csv', index=False, encoding='utf-8-sig')
        df.to_excel('data/enhanced_talent_policies.xlsx', index=False)
        
        # å¯¼å‡ºä¼ä¸šç”³æŠ¥è¦æ±‚è¯¦ç»†æ±‡æ€»
        with open('data/detailed_application_requirements.txt', 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("å¾æ±‡åŒºäººæ‰æ”¿ç­–ä¼ä¸šç”³æŠ¥è¦æ±‚è¯¦ç»†æ±‡æ€» (å¢å¼ºç‰ˆ)\n")
            f.write("=" * 80 + "\n\n")
            f.write(f"çˆ¬å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ€»æ”¿ç­–æ•°é‡: {len(self.policies)} æ¡\n")
            f.write(f"AIç›¸å…³æ”¿ç­–: {ai_policies} æ¡\n")
            f.write(f"åŒ…å«ç”³æŠ¥è¦æ±‚: {policies_with_requirements} æ¡\n\n")
            
            # æŒ‰åˆ†ç±»æ•´ç†
            for category in ['AI/äººå·¥æ™ºèƒ½', 'äººæ‰å¼•è¿›', 'åˆ›ä¸šæ‰¶æŒ', 'èµ„é‡‘è¡¥è´´', 'äº§ä¸šå‘å±•']:
                category_policies = [p for p in self.policies if p.get('category') == category]
                if category_policies:
                    f.write(f"\n{'='*20} {category} ({len(category_policies)}æ¡) {'='*20}\n\n")
                    
                    for i, policy in enumerate(category_policies, 1):
                        f.write(f"{i}. {policy.get('title', '')}\n")
                        f.write(f"   ğŸ“… å‘å¸ƒæ—¶é—´: {policy.get('publish_date', 'æœªçŸ¥')}\n")
                        f.write(f"   ğŸ›ï¸  å‘å¸ƒéƒ¨é—¨: {policy.get('department', 'æœªçŸ¥')}\n")
                        f.write(f"   ğŸ”— é“¾æ¥: {policy.get('url', '')}\n\n")
                        
                        req = policy.get('application_requirements', {})
                        for req_type, req_content in req.items():
                            if req_content.strip():
                                f.write(f"   ã€{req_type}ã€‘\n")
                                # åˆ†è¡Œæ˜¾ç¤ºï¼Œæé«˜å¯è¯»æ€§
                                items = req_content.split('; ')
                                for item in items:
                                    if item.strip():
                                        f.write(f"   â€¢ {item.strip()}\n")
                                f.write("\n")
                        
                        f.write("-" * 80 + "\n\n")
        
        # ç”Ÿæˆå¢å¼ºåˆ†ææŠ¥å‘Š
        with open('data/enhanced_analysis_report.txt', 'w', encoding='utf-8') as f:
            f.write("å¾æ±‡åŒºäººæ‰æ”¿ç­–æ·±åº¦åˆ†ææŠ¥å‘Š (å¢å¼ºç‰ˆ)\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"çˆ¬å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ€»æ”¿ç­–æ•°é‡: {len(self.policies)} æ¡\n")
            f.write(f"AIç›¸å…³æ”¿ç­–: {ai_policies} æ¡ ({ai_policies/len(self.policies)*100:.1f}%)\n")
            f.write(f"åŒ…å«ç”³æŠ¥è¦æ±‚çš„æ”¿ç­–: {policies_with_requirements} æ¡ ({policies_with_requirements/len(self.policies)*100:.1f}%)\n\n")
            
            f.write("ğŸ“Š æ”¿ç­–åˆ†ç±»åˆ†å¸ƒ:\n")
            for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
                percentage = count / len(self.policies) * 100
                f.write(f"   {cat}: {count}æ¡ ({percentage:.1f}%)\n")
            
            f.write(f"\nğŸ¢ ä¸»è¦å‘å¸ƒéƒ¨é—¨:\n")
            for dept, count in sorted(departments.items(), key=lambda x: x[1], reverse=True):
                if count > 0:
                    f.write(f"   {dept}: {count}æ¡\n")
            
            # AIæ”¿ç­–ç‰¹åˆ«åˆ†æ
            f.write(f"\nğŸ¤– AIæ”¿ç­–ç‰¹åˆ«åˆ†æ:\n")
            ai_policy_list = [p for p in self.policies if p.get('category') == 'AI/äººå·¥æ™ºèƒ½']
            for policy in ai_policy_list:
                f.write(f"   â€¢ {policy.get('title', '')}\n")
        
        print(f"\nğŸ“ å¢å¼ºç‰ˆæ•°æ®å·²å¯¼å‡ºåˆ° data/ ç›®å½•:")
        print(f"   ğŸ¯ è¯¦ç»†ç”³æŠ¥è¦æ±‚: data/detailed_application_requirements.txt")
        print(f"   ğŸ“Š æ·±åº¦åˆ†ææŠ¥å‘Š: data/enhanced_analysis_report.txt")
        print(f"   ğŸ“‹ å®Œæ•´æ•°æ®: data/enhanced_talent_policies.json")
        print(f"   ğŸ“Š è¡¨æ ¼æ•°æ®: data/enhanced_talent_policies.csv")
        print(f"   ğŸ“Š Excelæ–‡ä»¶: data/enhanced_talent_policies.xlsx")

def main():
    crawler = EnhancedXuhuiTalentCrawler()
    crawler.crawl_all_policies()
    crawler.analyze_and_export()
    
    print("\nğŸ¯ å¢å¼ºç‰ˆçˆ¬å–å®Œæˆï¼é‡ç‚¹æŸ¥çœ‹:")
    print("ğŸ“‹ è¯¦ç»†ä¼ä¸šç”³æŠ¥è¦æ±‚: data/detailed_application_requirements.txt")
    print("ğŸ“Š æ·±åº¦åˆ†ææŠ¥å‘Š: data/enhanced_analysis_report.txt")

if __name__ == "__main__":
    main() 