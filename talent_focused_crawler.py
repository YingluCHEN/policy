#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾æ±‡åŒºäººæ‰æ”¿ç­–ä¸“é¡¹çˆ¬è™«
ä¸“é—¨é’ˆå¯¹ä¼ä¸šäººæ‰å¼•è¿›ã€æˆ¿å±‹è¡¥è´´ã€è½æˆ·æœåŠ¡ç­‰æ”¿ç­–
"""

import requests
import re
import json
import time
import pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import urljoin, urlparse
import os
from concurrent.futures import ThreadPoolExecutor, as_completed

class TalentFocusedCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        
        # ä¸“é—¨é’ˆå¯¹äººæ‰æ”¿ç­–çš„URL
        self.urls = [
            # æ”¿åŠ¡å…¬å¼€ - æ›´å¤šé¡µé¢
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_xxgyhb&page=1",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_xxgyhb&page=2",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_xxgyhb&page=3",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_xxgyhb&page=4",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_xxgyhb&page=5",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_xxgyhb&page=6",
            
            # äººåŠ›èµ„æºå’Œç¤¾ä¼šä¿éšœå±€ - é‡ç‚¹æ‰©å±•
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_rshbj&page=1",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_rshbj&page=2",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_rshbj&page=3",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_rshbj&page=4",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_rshbj&page=5",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_rshbj&page=6",
            
            # ä½æˆ¿ä¿éšœå’Œæˆ¿å±‹ç®¡ç†å±€ - é‡ç‚¹å…³æ³¨
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_zfbzj&page=1",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_zfbzj&page=2",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_zfbzj&page=3",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_zfbzj&page=4",
            
            # ç§‘å­¦æŠ€æœ¯å§”å‘˜ä¼š
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_kjw&page=1",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_kjw&page=2",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_kjw&page=3",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_kjw&page=4",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_kjw&page=5",
            
            # æ•™è‚²å±€ - äººæ‰å­å¥³æ•™è‚²
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_jyj&page=1",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_jyj&page=2",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_jyj&page=3",
            
            # å«ç”Ÿå¥åº·å§”å‘˜ä¼š - äººæ‰åŒ»ç–—æœåŠ¡
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_wsjkw&page=1",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_wsjkw&page=2",
            
            # å…¬å®‰åˆ†å±€ - è½æˆ·æœåŠ¡
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_gafj&page=1",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_gafj&page=2",
            
            # é€šçŸ¥å…¬å‘Š - äººæ‰ç›¸å…³é€šçŸ¥
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_gggs&page=1",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_gggs&page=2",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_gggs&page=3",
            
            # è§„èŒƒæ€§æ–‡ä»¶
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_zcwj&page=1",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_zcwj&page=2",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_zcwj&page=3",
            
            # ä¸“é—¨çš„äººæ‰æ”¿ç­–æœç´¢
            "https://www.xuhui.gov.cn/search/pcRender?pageId=51bc01c2a0b04b5e8816d5eaea8c6df4&advance=true&tpl=2068&keyword=äººæ‰å¼•è¿›",
            "https://www.xuhui.gov.cn/search/pcRender?pageId=51bc01c2a0b04b5e8816d5eaea8c6df4&advance=true&tpl=2068&keyword=äººæ‰è¡¥è´´",
            "https://www.xuhui.gov.cn/search/pcRender?pageId=51bc01c2a0b04b5e8816d5eaea8c6df4&advance=true&tpl=2068&keyword=ä½æˆ¿è¡¥è´´",
            "https://www.xuhui.gov.cn/search/pcRender?pageId=51bc01c2a0b04b5e8816d5eaea8c6df4&advance=true&tpl=2068&keyword=äººæ‰å…¬å¯“",
            "https://www.xuhui.gov.cn/search/pcRender?pageId=51bc01c2a0b04b5e8816d5eaea8c6df4&advance=true&tpl=2068&keyword=è½æˆ·",
            "https://www.xuhui.gov.cn/search/pcRender?pageId=51bc01c2a0b04b5e8816d5eaea8c6df4&advance=true&tpl=2068&keyword=äººæ‰æœåŠ¡",
            "https://www.xuhui.gov.cn/search/pcRender?pageId=51bc01c2a0b04b5e8816d5eaea8c6df4&advance=true&tpl=2068&keyword=é«˜å±‚æ¬¡äººæ‰",
            "https://www.xuhui.gov.cn/search/pcRender?pageId=51bc01c2a0b04b5e8816d5eaea8c6df4&advance=true&tpl=2068&keyword=æµ·å¤–äººæ‰",
            "https://www.xuhui.gov.cn/search/pcRender?pageId=51bc01c2a0b04b5e8816d5eaea8c6df4&advance=true&tpl=2068&keyword=åšå£«",
            "https://www.xuhui.gov.cn/search/pcRender?pageId=51bc01c2a0b04b5e8816d5eaea8c6df4&advance=true&tpl=2068&keyword=ç¡•å£«",
        ]
        
        # ä¸“é—¨çš„äººæ‰æ”¿ç­–å…³é”®è¯
        self.talent_keywords = [
            # äººæ‰å¼•è¿›ç±»
            "äººæ‰å¼•è¿›", "äººæ‰æ‹›è˜", "äººæ‰è®¡åˆ’", "é«˜å±‚æ¬¡äººæ‰", "é¢†å†›äººæ‰", "æ‹”å°–äººæ‰",
            "é’å¹´äººæ‰", "æµ·å¤–äººæ‰", "å¤–ç±äººæ‰", "ä¸“å®¶", "å­¦è€…", "ç§‘å­¦å®¶",
            "åšå£«", "ç¡•å£«", "æ•™æˆ", "ç ”ç©¶å‘˜", "å·¥ç¨‹å¸ˆ", "é™¢å£«",
            "åƒäººè®¡åˆ’", "ä¸‡äººè®¡åˆ’", "æ°é’", "é•¿æ±Ÿå­¦è€…", "åšå£«å", "æµ·å½’", "ç•™å­¦äººå‘˜",
            
            # ä½æˆ¿ä¿éšœç±»
            "ä½æˆ¿è¡¥è´´", "æˆ¿å±‹è¡¥è´´", "ç§Ÿæˆ¿è¡¥è´´", "è´­æˆ¿è¡¥è´´", "ä½æˆ¿æ”¯æŒ", "ä½æˆ¿ä¼˜æƒ ",
            "äººæ‰å…¬å¯“", "äººæ‰æˆ¿", "ä¿éšœæ€§ä½æˆ¿", "å…¬ç§Ÿæˆ¿", "å»‰ç§Ÿæˆ¿", "ç§Ÿèµä½æˆ¿",
            "å®‰å±…å·¥ç¨‹", "ä½æˆ¿ä¿éšœ", "ç§Ÿé‡‘è¡¥è´´", "æˆ¿ç§Ÿè¡¥åŠ©",
            
            # è½æˆ·æœåŠ¡ç±»
            "è½æˆ·", "æˆ·ç±", "å±…ä½è¯", "ç§¯åˆ†è½æˆ·", "äººæ‰è½æˆ·", "ç›´æ¥è½æˆ·",
            "è¿æˆ·", "æˆ·å£", "å±…ä½è¯ç§¯åˆ†", "è½æˆ·æ”¿ç­–", "æˆ·ç±æ”¿ç­–",
            
            # å­å¥³æ•™è‚²ç±»
            "å­å¥³å…¥å­¦", "å­å¥³æ•™è‚²", "æ•™è‚²ä¼˜æƒ ", "å…¥å­¦æ”¿ç­–", "å­¦åŒº", "æ‹©æ ¡",
            "æ•™è‚²è¡¥è´´", "å­¦è´¹å‡å…", "å›½é™…å­¦æ ¡", "ä¼˜è´¨æ•™è‚²",
            
            # åŒ»ç–—ä¿éšœç±»
            "åŒ»ç–—ä¿éšœ", "åŒ»ç–—æœåŠ¡", "å¥åº·æœåŠ¡", "ä½“æ£€", "åŒ»ç–—ä¼˜æƒ ",
            "åŒ»ç–—è¡¥è´´", "å°±åŒ»ç»¿è‰²é€šé“", "å¥åº·ç®¡ç†",
            
            # é…å¶å°±ä¸šç±»
            "é…å¶å°±ä¸š", "å®¶å±å°±ä¸š", "å°±ä¸šæ”¯æŒ", "èŒä¸šä»‹ç»", "å°±ä¸šæœåŠ¡",
            
            # åˆ›ä¸šæ”¯æŒç±»
            "åˆ›ä¸šæ”¯æŒ", "åˆ›ä¸šè¡¥è´´", "åˆ›ä¸šå­µåŒ–", "åˆ›ä¸šåŸºé‡‘", "åˆ›ä¸šæ‰¶æŒ",
            "åˆåˆ›ä¼ä¸š", "åˆ›ä¸šå›­", "ä¼—åˆ›ç©ºé—´",
            
            # èµ„é‡‘æ”¯æŒç±»
            "äººæ‰èµ„åŠ©", "äººæ‰å¥–åŠ±", "ç§‘ç ”èµ„åŠ©", "é¡¹ç›®èµ„åŠ©", "å¯åŠ¨èµ„é‡‘",
            "æ´¥è´´", "è¡¥åŠ©", "å¥–é‡‘", "æ¿€åŠ±", "æ‰¶æŒèµ„é‡‘",
            
            # ç”³æŠ¥ç›¸å…³
            "ç”³æŠ¥", "ç”³è¯·", "æ¡ä»¶", "è¦æ±‚", "ææ–™", "æµç¨‹", "åŠç†",
            "å®¡æ ¸", "è¯„å®¡", "è®¤å®š", "èµ„æ ¼", "æ ‡å‡†"
        ]
        
        # ä¼ä¸šäººæ‰æ”¿ç­–ç”³æŠ¥æ¡ä»¶å…³é”®è¯
        self.application_keywords = {
            "ä¼ä¸šåŸºæœ¬æ¡ä»¶": [
                "æ³¨å†Œåœ°", "æ³¨å†Œèµ„æœ¬", "å®ç¼´èµ„æœ¬", "ç»è¥æœŸé™", "è¥ä¸šæ‰§ç…§",
                "ä¼ä¸šæ€§è´¨", "æ‰€æœ‰åˆ¶", "è¡Œä¸šç±»åˆ«", "ç»è¥èŒƒå›´", "æˆç«‹æ—¶é—´",
                "è¥ä¸šæ”¶å…¥", "çº³ç¨é¢", "å‘˜å·¥è§„æ¨¡", "æŠ€æœ¯äººå‘˜æ¯”ä¾‹", "ç ”å‘æŠ•å…¥",
                "é«˜æ–°æŠ€æœ¯ä¼ä¸š", "ä¸“ç²¾ç‰¹æ–°", "ç‹¬è§’å…½ä¼ä¸š", "ä¸Šå¸‚å…¬å¸"
            ],
            "äººæ‰æ¡ä»¶è¦æ±‚": [
                "å­¦å†è¦æ±‚", "å­¦ä½è¦æ±‚", "ä¸“ä¸šè¦æ±‚", "å·¥ä½œç»éªŒ", "å¹´é¾„é™åˆ¶",
                "æŠ€æœ¯èŒç§°", "ä¸“ä¸šæŠ€èƒ½", "è¯­è¨€èƒ½åŠ›", "æµ·å¤–ç»å†", "å·¥ä½œä¸šç»©",
                "ç§‘ç ”æˆæœ", "ä¸“åˆ©å‘æ˜", "è·å¥–æƒ…å†µ", "æ¨èä¿¡", "å­¦æœ¯å£°èª‰"
            ],
            "ç”³æŠ¥ææ–™æ¸…å•": [
                "ç”³æŠ¥è¡¨", "ç”³è¯·è¡¨", "èº«ä»½è¯æ˜", "å­¦å†è¯æ˜", "å­¦ä½è¯ä¹¦",
                "å·¥ä½œè¯æ˜", "æ¨èä¿¡", "ç®€å†", "ä¸šç»©ææ–™", "è·å¥–è¯ä¹¦",
                "è´¢åŠ¡æŠ¥è¡¨", "çº³ç¨è¯æ˜", "ä¼ä¸šèµ„è´¨", "ä¸“åˆ©è¯ä¹¦", "è®ºæ–‡è‘—ä½œ"
            ],
            "ç”³æŠ¥æ—¶é—´å®‰æ’": [
                "ç”³æŠ¥æ—¶é—´", "æˆªæ­¢æ—¶é—´", "å—ç†æœŸé—´", "è¯„å®¡æ—¶é—´", "å…¬ç¤ºæ—¶é—´",
                "åŠç†æ—¶é™", "æœ‰æ•ˆæœŸ", "å¹´åº¦ç”³æŠ¥", "æ‰¹æ¬¡ç”³æŠ¥", "å¸¸å¹´å—ç†"
            ],
            "èµ„é‡‘æ”¯æŒæ ‡å‡†": [
                "è¡¥è´´æ ‡å‡†", "å¥–åŠ±é‡‘é¢", "æ”¯æŒé¢åº¦", "æœ€é«˜é™é¢", "åˆ†çº§æ”¯æŒ",
                "ä¸€æ¬¡æ€§", "æŒ‰å¹´åº¦", "åˆ†æœŸæ‹¨ä»˜", "é…å¥—èµ„é‡‘", "ä¸“é¡¹èµ„é‡‘"
            ],
            "ä½æˆ¿æ”¯æŒæ”¿ç­–": [
                "ä½æˆ¿è¡¥è´´", "ç§Ÿé‡‘è¡¥è´´", "è´­æˆ¿ä¼˜æƒ ", "äººæ‰å…¬å¯“", "å…è´¹å±…ä½",
                "ç§Ÿé‡‘å‡å…", "è´­æˆ¿èµ„åŠ©", "ä½æˆ¿åˆ†é…", "ä¼˜å…ˆè´­æˆ¿", "ç§Ÿèµè¡¥è´´"
            ],
            "è½æˆ·æœåŠ¡æ¡ä»¶": [
                "è½æˆ·æ¡ä»¶", "ç§¯åˆ†è¦æ±‚", "å±…ä½è¯", "ç¤¾ä¿ç¼´è´¹", "çº³ç¨è®°å½•",
                "è½æˆ·æµç¨‹", "åŠç†ææ–™", "å®¡æ‰¹æ—¶é—´", "é…å¶è½æˆ·", "å­å¥³è½æˆ·"
            ],
            "å…¶ä»–é…å¥—æœåŠ¡": [
                "å­å¥³å…¥å­¦", "åŒ»ç–—æœåŠ¡", "é…å¶å°±ä¸š", "åˆ›ä¸šæ”¯æŒ", "åŸ¹è®­æœºä¼š",
                "å­¦æœ¯äº¤æµ", "èŒä¸šå‘å±•", "ç»¿è‰²é€šé“", "ä¸€ç«™å¼æœåŠ¡", "ä¸“å‘˜æœåŠ¡"
            ]
        }
        
        self.policies = []

    def fetch_page(self, url):
        """è·å–é¡µé¢å†…å®¹"""
        try:
            print(f"æ­£åœ¨çˆ¬å–: {url}")
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return response.text
        except Exception as e:
            print(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
            return None

    def extract_policy_links(self, html, base_url):
        """ä»åˆ—è¡¨é¡µæå–æ”¿ç­–é“¾æ¥ - äººæ‰æ”¿ç­–ä¸“ç”¨ç‰ˆ"""
        if not html:
            return []
        
        soup = BeautifulSoup(html, 'html.parser')
        links = []
        
        # å¤šç§é€‰æ‹©å™¨ç­–ç•¥
        link_selectors = [
            'a[href*="detail"]',
            'a[href*="article"]',
            '.list-item a',
            '.title a',
            'li a',
            '.policy-title a',
            '.news-title a'
        ]
        
        found_links = []
        for selector in link_selectors:
            found_links.extend(soup.select(selector))
        
        # äººæ‰æ”¿ç­–è¯†åˆ«
        seen_hrefs = set()
        for a in found_links:
            href = a.get('href')
            title = a.get_text(strip=True)
            
            if href and href not in seen_hrefs and len(title) > 5:
                # æ›´ç²¾ç¡®çš„äººæ‰æ”¿ç­–è¯†åˆ«
                is_talent_related = any(keyword in title for keyword in self.talent_keywords)
                
                if is_talent_related:
                    full_url = urljoin(base_url, href)
                    links.append({
                        'title': title,
                        'url': full_url
                    })
                    seen_hrefs.add(href)
        
        return links

    def extract_policy_content(self, url):
        """æå–æ”¿ç­–è¯¦ç»†å†…å®¹"""
        html = self.fetch_page(url)
        if not html:
            return None
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # æå–æ ‡é¢˜
        title = ""
        title_selectors = [
            'h1', 'h2', '.title', '.art-title', '.page-title', 
            '[class*="title"]', '[class*="head"]', '.main-title'
        ]
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem and len(title_elem.get_text(strip=True)) > 5:
                title = title_elem.get_text(strip=True)
                break
        
        # æå–å†…å®¹
        content = ""
        content_selectors = [
            '.content', '.art-content', '.main-content', '.policy-content',
            '[class*="content"]', '.main', 'article', '.detail', '.text'
        ]
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                for nav in content_elem.select('nav, .nav, .menu, .breadcrumb, .pagination'):
                    nav.decompose()
                content = content_elem.get_text(strip=True)
                if len(content) > 200:
                    break
        
        if not content or len(content) < 100:
            body = soup.find('body')
            if body:
                for elem in body.select('nav, .nav, .menu, .sidebar, .footer, .header'):
                    elem.decompose()
                content = body.get_text(strip=True)
        
        # æå–å‘å¸ƒæ—¶é—´
        publish_date = ""
        date_patterns = [
            r'(\d{4}[-å¹´]\d{1,2}[-æœˆ]\d{1,2}[æ—¥]?)',
            r'(\d{4}/\d{1,2}/\d{1,2})',
            r'(\d{4}\.\d{1,2}\.\d{1,2})',
            r'å‘å¸ƒæ—¶é—´[ï¼š:]\s*(\d{4}[-å¹´]\d{1,2}[-æœˆ]\d{1,2})',
            r'æ—¶é—´[ï¼š:]\s*(\d{4}[-å¹´]\d{1,2}[-æœˆ]\d{1,2})'
        ]
        
        search_text = content + title + str(soup)
        for pattern in date_patterns:
            match = re.search(pattern, search_text)
            if match:
                publish_date = match.group(1)
                break
        
        # æå–å‘å¸ƒéƒ¨é—¨
        department = ""
        dept_patterns = [
            r'å‘å¸ƒæœºæ„[ï¼š:]\s*([^ï¼Œã€‚ï¼›ï¼ï¼Ÿ\n]+)',
            r'å‘å¸ƒéƒ¨é—¨[ï¼š:]\s*([^ï¼Œã€‚ï¼›ï¼ï¼Ÿ\n]+)',
            r'(å¾æ±‡åŒº[^ï¼Œã€‚ï¼›ï¼ï¼Ÿ\n]*?[å±€|å§”|åŠ|ä¸­å¿ƒ|éƒ¨|å¤„])',
            r'(ä¸Šæµ·å¸‚å¾æ±‡åŒº[^ï¼Œã€‚ï¼›ï¼ï¼Ÿ\n]*?[å±€|å§”|åŠ|ä¸­å¿ƒ|éƒ¨|å¤„])'
        ]
        
        for pattern in dept_patterns:
            match = re.search(pattern, search_text)
            if match:
                department = match.group(1).strip()
                break
        
        return {
            'title': title,
            'url': url,
            'content': content[:6000],
            'publish_date': publish_date,
            'department': department,
            'crawl_time': datetime.now().isoformat()
        }

    def classify_policy(self, policy):
        """äººæ‰æ”¿ç­–åˆ†ç±»"""
        text = f"{policy['title']} {policy['content']}"
        
        categories = {
            "äººæ‰å¼•è¿›": [
                "äººæ‰å¼•è¿›", "äººæ‰æ‹›è˜", "æµ·å¤–äººæ‰", "é«˜å±‚æ¬¡äººæ‰", "é¢†å†›äººæ‰",
                "ä¸“å®¶", "åšå£«", "ç¡•å£«", "é™¢å£«", "åƒäººè®¡åˆ’", "ä¸‡äººè®¡åˆ’"
            ],
            "ä½æˆ¿ä¿éšœ": [
                "ä½æˆ¿è¡¥è´´", "æˆ¿å±‹è¡¥è´´", "ç§Ÿæˆ¿è¡¥è´´", "äººæ‰å…¬å¯“", "ä½æˆ¿æ”¯æŒ",
                "ä¿éšœæ€§ä½æˆ¿", "å…¬ç§Ÿæˆ¿", "å»‰ç§Ÿæˆ¿", "å®‰å±…", "ç§Ÿé‡‘"
            ],
            "è½æˆ·æœåŠ¡": [
                "è½æˆ·", "æˆ·ç±", "å±…ä½è¯", "ç§¯åˆ†", "è¿æˆ·", "æˆ·å£", "è½æˆ·æ”¿ç­–"
            ],
            "å­å¥³æ•™è‚²": [
                "å­å¥³å…¥å­¦", "å­å¥³æ•™è‚²", "æ•™è‚²ä¼˜æƒ ", "å…¥å­¦", "å­¦åŒº", "æ‹©æ ¡"
            ],
            "åŒ»ç–—ä¿éšœ": [
                "åŒ»ç–—ä¿éšœ", "åŒ»ç–—æœåŠ¡", "å¥åº·", "ä½“æ£€", "åŒ»ç–—ä¼˜æƒ ", "å°±åŒ»"
            ],
            "åˆ›ä¸šæ”¯æŒ": [
                "åˆ›ä¸šæ”¯æŒ", "åˆ›ä¸šè¡¥è´´", "åˆ›ä¸šå­µåŒ–", "åˆ›ä¸šåŸºé‡‘", "åˆåˆ›", "ä¼—åˆ›"
            ],
            "èµ„é‡‘èµ„åŠ©": [
                "äººæ‰èµ„åŠ©", "äººæ‰å¥–åŠ±", "ç§‘ç ”èµ„åŠ©", "é¡¹ç›®èµ„åŠ©", "æ´¥è´´", "è¡¥åŠ©"
            ],
            "é…å¶å°±ä¸š": [
                "é…å¶å°±ä¸š", "å®¶å±å°±ä¸š", "å°±ä¸šæ”¯æŒ", "èŒä¸šä»‹ç»"
            ]
        }
        
        # è®¡ç®—åˆ†ç±»å¾—åˆ†
        category_scores = {}
        for category, keywords in categories.items():
            score = 0
            for keyword in keywords:
                if keyword in policy['title']:
                    score += 3
                if keyword in policy['content']:
                    score += 1
            category_scores[category] = score
        
        if category_scores:
            best_category = max(category_scores, key=category_scores.get)
            if category_scores[best_category] > 0:
                return best_category
        
        return "å…¶ä»–"

    def extract_application_requirements(self, policy):
        """æå–ä¼ä¸šäººæ‰æ”¿ç­–ç”³æŠ¥è¦æ±‚"""
        text = f"{policy['title']} {policy['content']}"
        requirements = {}
        
        for req_type, keywords in self.application_keywords.items():
            found_requirements = []
            
            for keyword in keywords:
                sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›\n]', text)
                for sentence in sentences:
                    if keyword in sentence and len(sentence.strip()) > 10:
                        cleaned = sentence.strip()
                        if (len(cleaned) > 15 and 
                            not cleaned.startswith(('é¦–é¡µ', 'è¿”å›', 'ä¸Šä¸€é¡µ', 'ä¸‹ä¸€é¡µ')) and
                            not all(c in '=- \t' for c in cleaned)):
                            found_requirements.append(cleaned)
            
            # å»é‡å¹¶é™åˆ¶æ•°é‡
            unique_requirements = []
            seen = set()
            for req in found_requirements:
                if req not in seen and len(req) < 400:
                    unique_requirements.append(req)
                    seen.add(req)
                if len(unique_requirements) >= 6:
                    break
            
            requirements[req_type] = "; ".join(unique_requirements) if unique_requirements else ""
        
        return requirements

    def crawl_talent_policies(self):
        """çˆ¬å–äººæ‰æ”¿ç­–"""
        print("å¼€å§‹ä¸“é¡¹çˆ¬å–å¾æ±‡åŒºä¼ä¸šäººæ‰æ”¿ç­–...")
        print(f"å°†çˆ¬å– {len(self.urls)} ä¸ªäººæ‰æ”¿ç­–ä¸“ç”¨URLæº")
        
        os.makedirs('data', exist_ok=True)
        
        all_links = []
        
        # å¹¶å‘çˆ¬å–
        def fetch_links(url):
            html = self.fetch_page(url)
            if html:
                return self.extract_policy_links(html, url)
            return []
        
        with ThreadPoolExecutor(max_workers=3) as executor:
            future_to_url = {executor.submit(fetch_links, url): url for url in self.urls}
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    links = future.result()
                    all_links.extend(links)
                    if links:
                        print(f"ä» {url} è·å–åˆ° {len(links)} ä¸ªäººæ‰æ”¿ç­–é“¾æ¥")
                except Exception as e:
                    print(f"å¤„ç† {url} æ—¶å‡ºé”™: {e}")
                time.sleep(0.5)
        
        # å»é‡
        unique_links = []
        seen_urls = set()
        for link in all_links:
            if link['url'] not in seen_urls:
                unique_links.append(link)
                seen_urls.add(link['url'])
        
        print(f"å»é‡åå…±æ‰¾åˆ° {len(unique_links)} ä¸ªäººæ‰æ”¿ç­–é“¾æ¥")
        
        # çˆ¬å–è¯¦ç»†å†…å®¹ï¼Œå¢åŠ åˆ°60ä¸ª
        target_links = unique_links[:60]
        print(f"å°†è¯¦ç»†çˆ¬å–å‰ {len(target_links)} ä¸ªäººæ‰æ”¿ç­–")
        
        for i, link in enumerate(target_links, 1):
            print(f"æ­£åœ¨å¤„ç†ç¬¬ {i}/{len(target_links)} ä¸ªæ”¿ç­–: {link['title'][:60]}...")
            
            policy = self.extract_policy_content(link['url'])
            if policy and len(policy['content']) > 150:
                policy['category'] = self.classify_policy(policy)
                policy['application_requirements'] = self.extract_application_requirements(policy)
                
                # ä¿ç•™äººæ‰ç›¸å…³æ”¿ç­–
                req = policy['application_requirements']
                has_talent_req = any(
                    len(v.strip()) > 15 for v in req.values() if v
                )
                
                talent_categories = ['äººæ‰å¼•è¿›', 'ä½æˆ¿ä¿éšœ', 'è½æˆ·æœåŠ¡', 'å­å¥³æ•™è‚²', 'åŒ»ç–—ä¿éšœ', 'åˆ›ä¸šæ”¯æŒ', 'èµ„é‡‘èµ„åŠ©', 'é…å¶å°±ä¸š']
                
                if has_talent_req or policy['category'] in talent_categories:
                    self.policies.append(policy)
                    print(f"  âœ… å·²ä¿å­˜ (åˆ†ç±»: {policy['category']})")
                else:
                    print(f"  âŒ è·³è¿‡ (éäººæ‰æ”¿ç­–)")
            
            time.sleep(1.2)
        
        print(f"\nğŸ¯ æˆåŠŸçˆ¬å– {len(self.policies)} ä¸ªäººæ‰æ”¿ç­–")

    def analyze_and_export(self):
        """åˆ†æå¹¶å¯¼å‡ºäººæ‰æ”¿ç­–æ•°æ®"""
        if not self.policies:
            print("æ²¡æœ‰çˆ¬å–åˆ°æœ‰æ•ˆæ•°æ®")
            return
        
        print("æ­£åœ¨åˆ†æäººæ‰æ”¿ç­–æ•°æ®...")
        
        # ç»Ÿè®¡åˆ†æ
        categories = {}
        departments = {}
        policies_with_requirements = 0
        
        for policy in self.policies:
            cat = policy.get('category', 'å…¶ä»–')
            categories[cat] = categories.get(cat, 0) + 1
            
            dept = policy.get('department', 'æœªçŸ¥')
            departments[dept] = departments.get(dept, 0) + 1
            
            req = policy.get('application_requirements', {})
            if any(v.strip() for v in req.values() if v):
                policies_with_requirements += 1
        
        # å¯¼å‡ºJSON
        output_data = {
            'crawl_time': datetime.now().isoformat(),
            'total_policies': len(self.policies),
            'statistics': {
                'categories': categories,
                'departments': departments,
                'policies_with_requirements': policies_with_requirements
            },
            'policies': self.policies
        }
        
        with open('data/talent_policies.json', 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        # å¯¼å‡ºCSV
        df_data = []
        for policy in self.policies:
            req = policy.get('application_requirements', {})
            df_data.append({
                'æ ‡é¢˜': policy.get('title', ''),
                'åˆ†ç±»': policy.get('category', ''),
                'å‘å¸ƒéƒ¨é—¨': policy.get('department', ''),
                'å‘å¸ƒæ—¶é—´': policy.get('publish_date', ''),
                'é“¾æ¥': policy.get('url', ''),
                'ä¼ä¸šåŸºæœ¬æ¡ä»¶': req.get('ä¼ä¸šåŸºæœ¬æ¡ä»¶', ''),
                'äººæ‰æ¡ä»¶è¦æ±‚': req.get('äººæ‰æ¡ä»¶è¦æ±‚', ''),
                'ç”³æŠ¥ææ–™æ¸…å•': req.get('ç”³æŠ¥ææ–™æ¸…å•', ''),
                'ç”³æŠ¥æ—¶é—´å®‰æ’': req.get('ç”³æŠ¥æ—¶é—´å®‰æ’', ''),
                'èµ„é‡‘æ”¯æŒæ ‡å‡†': req.get('èµ„é‡‘æ”¯æŒæ ‡å‡†', ''),
                'ä½æˆ¿æ”¯æŒæ”¿ç­–': req.get('ä½æˆ¿æ”¯æŒæ”¿ç­–', ''),
                'è½æˆ·æœåŠ¡æ¡ä»¶': req.get('è½æˆ·æœåŠ¡æ¡ä»¶', ''),
                'å…¶ä»–é…å¥—æœåŠ¡': req.get('å…¶ä»–é…å¥—æœåŠ¡', ''),
                'å†…å®¹æ‘˜è¦': policy.get('content', '')[:300]
            })
        
        df = pd.DataFrame(df_data)
        df.to_csv('data/talent_policies.csv', index=False, encoding='utf-8-sig')
        df.to_excel('data/talent_policies.xlsx', index=False)
        
        # å¯¼å‡ºè¯¦ç»†ç”³æŠ¥è¦æ±‚
        with open('data/talent_application_requirements.txt', 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("å¾æ±‡åŒºä¼ä¸šäººæ‰æ”¿ç­–ç”³æŠ¥è¦æ±‚è¯¦ç»†æ±‡æ€»\n")
            f.write("=" * 80 + "\n\n")
            f.write(f"çˆ¬å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ€»æ”¿ç­–æ•°é‡: {len(self.policies)} æ¡\n")
            f.write(f"åŒ…å«ç”³æŠ¥è¦æ±‚: {policies_with_requirements} æ¡\n\n")
            
            # æŒ‰åˆ†ç±»æ•´ç†
            for category in ['äººæ‰å¼•è¿›', 'ä½æˆ¿ä¿éšœ', 'è½æˆ·æœåŠ¡', 'å­å¥³æ•™è‚²', 'åŒ»ç–—ä¿éšœ', 'åˆ›ä¸šæ”¯æŒ', 'èµ„é‡‘èµ„åŠ©', 'é…å¶å°±ä¸š']:
                category_policies = [p for p in self.policies if p.get('category') == category]
                if category_policies:
                    f.write(f"\n{'='*15} {category} ({len(category_policies)}æ¡) {'='*15}\n\n")
                    
                    for i, policy in enumerate(category_policies, 1):
                        f.write(f"{i}. {policy.get('title', '')}\n")
                        f.write(f"   ğŸ“… å‘å¸ƒæ—¶é—´: {policy.get('publish_date', 'æœªçŸ¥')}\n")
                        f.write(f"   ğŸ›ï¸  å‘å¸ƒéƒ¨é—¨: {policy.get('department', 'æœªçŸ¥')}\n")
                        f.write(f"   ğŸ”— é“¾æ¥: {policy.get('url', '')}\n\n")
                        
                        req = policy.get('application_requirements', {})
                        for req_type, req_content in req.items():
                            if req_content.strip():
                                f.write(f"   ã€{req_type}ã€‘\n")
                                items = req_content.split('; ')
                                for item in items:
                                    if item.strip():
                                        f.write(f"   â€¢ {item.strip()}\n")
                                f.write("\n")
                        
                        f.write("-" * 70 + "\n\n")
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        with open('data/talent_analysis_report.txt', 'w', encoding='utf-8') as f:
            f.write("å¾æ±‡åŒºä¼ä¸šäººæ‰æ”¿ç­–åˆ†ææŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"çˆ¬å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ€»æ”¿ç­–æ•°é‡: {len(self.policies)} æ¡\n")
            f.write(f"åŒ…å«ç”³æŠ¥è¦æ±‚çš„æ”¿ç­–: {policies_with_requirements} æ¡ ({policies_with_requirements/len(self.policies)*100:.1f}%)\n\n")
            
            f.write("ğŸ“Š æ”¿ç­–åˆ†ç±»åˆ†å¸ƒ:\n")
            for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
                percentage = count / len(self.policies) * 100
                f.write(f"   {cat}: {count}æ¡ ({percentage:.1f}%)\n")
            
            f.write(f"\nğŸ¢ ä¸»è¦å‘å¸ƒéƒ¨é—¨:\n")
            for dept, count in sorted(departments.items(), key=lambda x: x[1], reverse=True):
                if count > 0:
                    f.write(f"   {dept}: {count}æ¡\n")
        
        print(f"\nğŸ“ äººæ‰æ”¿ç­–æ•°æ®å·²å¯¼å‡º:")
        print(f"   ğŸ¯ è¯¦ç»†ç”³æŠ¥è¦æ±‚: data/talent_application_requirements.txt")
        print(f"   ğŸ“Š åˆ†ææŠ¥å‘Š: data/talent_analysis_report.txt")
        print(f"   ğŸ“‹ å®Œæ•´æ•°æ®: data/talent_policies.json")
        print(f"   ğŸ“Š è¡¨æ ¼æ•°æ®: data/talent_policies.csv")
        print(f"   ğŸ“Š Excelæ–‡ä»¶: data/talent_policies.xlsx")

def main():
    crawler = TalentFocusedCrawler()
    crawler.crawl_talent_policies()
    crawler.analyze_and_export()
    
    print("\nğŸ¯ äººæ‰æ”¿ç­–ä¸“é¡¹çˆ¬å–å®Œæˆï¼")
    print("ğŸ“‹ é‡ç‚¹æŸ¥çœ‹: data/talent_application_requirements.txt")
    print("ğŸ“Š åˆ†ææŠ¥å‘Š: data/talent_analysis_report.txt")

if __name__ == "__main__":
    main() 