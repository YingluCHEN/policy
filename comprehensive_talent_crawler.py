#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾æ±‡åŒºä¼ä¸šäººæ‰æ”¿ç­–ç»¼åˆçˆ¬è™«
è¦†ç›–æ›´å…¨é¢çš„æ”¿ç­–æ¥æºï¼Œé‡ç‚¹å…³æ³¨ä¼ä¸šç”³æŠ¥è¦æ±‚
"""

import requests
import re
import json
import time
import pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import urljoin, urlparse
import os
from concurrent.futures import ThreadPoolExecutor, as_completed

class ComprehensiveTalentCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        
        # æ›´å…¨é¢çš„å¾æ±‡åŒºäººæ‰æ”¿ç­–URL
        self.urls = [
            # å¾æ±‡åŒºæ”¿åºœä¸»ç«™ - æ”¿ç­–æ³•è§„
            "https://www.xuhui.gov.cn/zcfg/",
            "https://www.xuhui.gov.cn/zcfg/sfxwj/",
            "https://www.xuhui.gov.cn/zcfg/qfxwj/",
            
            # æ”¿åŠ¡å…¬å¼€ - è§„èŒƒæ€§æ–‡ä»¶
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_zcwj",
            
            # äººåŠ›èµ„æºå’Œç¤¾ä¼šä¿éšœå±€
            "https://www.xuhui.gov.cn/renshebao/",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_rshbj",
            
            # ä½æˆ¿ä¿éšœå’Œæˆ¿å±‹ç®¡ç†å±€
            "https://www.xuhui.gov.cn/fangtuju/",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_zfbzj",
            
            # ç§‘å­¦æŠ€æœ¯å§”å‘˜ä¼š
            "https://www.xuhui.gov.cn/kejiju/",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_kjw",
            
            # æ•™è‚²å±€
            "https://www.xuhui.gov.cn/jiaoyu/",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_jyj",
            
            # å‘å±•å’Œæ”¹é©å§”å‘˜ä¼š
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_fgw",
            
            # å•†åŠ¡å§”å‘˜ä¼š
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_sww",
            
            # é€šçŸ¥å…¬å‘Š
            "https://www.xuhui.gov.cn/tzgg/",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_gggs",
            
            # ç›´æ¥æœç´¢äººæ‰æ”¿ç­–ç›¸å…³é¡µé¢
            "https://www.xuhui.gov.cn/ztlm/",  # ä¸“é¢˜ä¸“æ 
            "https://www.xuhui.gov.cn/zdly/",  # é‡ç‚¹é¢†åŸŸ
            
            # å¾æ±‡åŒºæ–°å‹å·¥ä¸šåŒ–æ¨è¿›åŠå…¬å®¤
            "https://www.xuhui.gov.cn/xgybgs/",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_xgybgs",
            
            # æ›´å¤šçš„æ”¿ç­–é¡µé¢
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_xxgyhb&page=1",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_xxgyhb&page=2",
            "https://www.xuhui.gov.cn/xxgk/portal/article/organizationArticle?code=xhxxgk_wbj_xxgyhb&page=3",
        ]
        
        # æ›´è¯¦ç»†çš„äººæ‰å…³é”®è¯
        self.talent_keywords = [
            # ç›´æ¥äººæ‰æ”¿ç­–
            "äººæ‰æ”¿ç­–", "äººæ‰æœåŠ¡", "äººæ‰è®¡åˆ’", "äººæ‰å·¥ç¨‹", "äººæ‰é¡¹ç›®",
            "äººæ‰å¼•è¿›", "äººæ‰æ‹›è˜", "äººæ‰åŸ¹å…»", "äººæ‰è¯„ä»·", "äººæ‰æ¿€åŠ±",
            
            # é«˜ç«¯äººæ‰
            "é«˜å±‚æ¬¡äººæ‰", "é¢†å†›äººæ‰", "æ‹”å°–äººæ‰", "éª¨å¹²äººæ‰", "é’å¹´äººæ‰",
            "æµ·å¤–äººæ‰", "å¤–ç±äººæ‰", "å›½é™…äººæ‰", "ç•™å­¦äººå‘˜", "æµ·å½’",
            
            # å­¦å†èŒç§°
            "åšå£«", "ç¡•å£«", "æ•™æˆ", "ç ”ç©¶å‘˜", "é«˜çº§å·¥ç¨‹å¸ˆ", "ä¸“å®¶",
            "é™¢å£«", "å­¦è€…", "ç§‘å­¦å®¶", "æŠ€æœ¯äººå‘˜", "ç ”å‘äººå‘˜",
            
            # äººæ‰è®¡åˆ’
            "åƒäººè®¡åˆ’", "ä¸‡äººè®¡åˆ’", "æ°é’", "é•¿æ±Ÿå­¦è€…", "åšå£«å",
            "ä¼˜ç§€äººæ‰", "ç´§ç¼ºäººæ‰", "ç‰¹æ®Šäººæ‰", "åˆ›æ–°äººæ‰", "åˆ›ä¸šäººæ‰",
            
            # ä½æˆ¿ç›¸å…³
            "äººæ‰å…¬å¯“", "äººæ‰æˆ¿", "ä½æˆ¿è¡¥è´´", "ç§Ÿæˆ¿è¡¥è´´", "è´­æˆ¿è¡¥è´´",
            "ä½æˆ¿æ”¯æŒ", "ä½æˆ¿ä¼˜æƒ ", "å®‰å±…å·¥ç¨‹", "ä¿éšœæ€§ä½æˆ¿", "å…¬ç§Ÿæˆ¿",
            "å»‰ç§Ÿæˆ¿", "ç§Ÿèµä½æˆ¿", "ä½æˆ¿ä¿éšœ", "ç§Ÿé‡‘è¡¥è´´", "æˆ¿ç§Ÿè¡¥åŠ©",
            
            # è½æˆ·ç›¸å…³
            "äººæ‰è½æˆ·", "ç›´æ¥è½æˆ·", "è½æˆ·æ”¿ç­–", "æˆ·ç±æ”¿ç­–", "å±…ä½è¯",
            "ç§¯åˆ†è½æˆ·", "å±…ä½è¯ç§¯åˆ†", "æˆ·å£è¿ç§»", "æˆ·ç±ç®¡ç†",
            
            # å­å¥³æ•™è‚²
            "å­å¥³å…¥å­¦", "å­å¥³æ•™è‚²", "æ•™è‚²ä¼˜æƒ ", "å…¥å­¦æ”¿ç­–", "æ‹©æ ¡",
            "æ•™è‚²æœåŠ¡", "å­¦åŒº", "å›½é™…å­¦æ ¡", "ä¼˜è´¨æ•™è‚²èµ„æº",
            
            # é…å¥—æœåŠ¡
            "åŒ»ç–—ä¿éšœ", "åŒ»ç–—æœåŠ¡", "å¥åº·æœåŠ¡", "é…å¶å°±ä¸š", "å®¶å±å®‰ç½®",
            "ç»¿è‰²é€šé“", "ä¸€ç«™å¼æœåŠ¡", "äººæ‰æœåŠ¡ä¸­å¿ƒ", "äººæ‰é©¿ç«™",
            
            # èµ„é‡‘æ”¯æŒ
            "äººæ‰èµ„åŠ©", "äººæ‰è¡¥è´´", "äººæ‰å¥–åŠ±", "åˆ›ä¸šè¡¥è´´", "ç§‘ç ”èµ„åŠ©",
            "é¡¹ç›®èµ„åŠ©", "å¯åŠ¨èµ„é‡‘", "æ´¥è´´", "å®‰å®¶è´¹", "ç”Ÿæ´»è¡¥è´´",
            
            # ä¼ä¸šäººæ‰
            "ä¼ä¸šäººæ‰", "ç”¨äººå•ä½", "äººæ‰å¼•è¿›ä¼ä¸š", "é›‡ä¸»", "æ‹›è˜å•ä½"
        ]
        
        # ä¼ä¸šç”³æŠ¥æ¡ä»¶å…³é”®è¯
        self.company_keywords = [
            "ä¼ä¸šæ³¨å†Œ", "æ³¨å†Œåœ°", "æ³¨å†Œèµ„æœ¬", "è¥ä¸šæ‰§ç…§", "ç»è¥æœŸé™",
            "ä¼ä¸šæ€§è´¨", "è¡Œä¸šç±»åˆ«", "ç»è¥èŒƒå›´", "çº³ç¨", "ä¿¡ç”¨",
            "é«˜æ–°æŠ€æœ¯ä¼ä¸š", "ä¸“ç²¾ç‰¹æ–°", "è§„æ¨¡ä»¥ä¸Š", "ä¸Šå¸‚å…¬å¸",
            "ç”³æŠ¥æ¡ä»¶", "ç”³è¯·æ¡ä»¶", "ç”³æŠ¥è¦æ±‚", "ç”³æŠ¥ææ–™", "ç”³æŠ¥æµç¨‹",
            "ä¼ä¸šè¦æ±‚", "ç”¨äººå•ä½æ¡ä»¶", "é›‡ä¸»æ¡ä»¶", "æ‹›è˜ä¼ä¸š"
        ]
        
        self.policies = []

    def fetch_page(self, url):
        """è·å–é¡µé¢å†…å®¹"""
        try:
            print(f"æ­£åœ¨çˆ¬å–: {url}")
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return response.text
        except Exception as e:
            print(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
            return None

    def extract_links_from_page(self, html, base_url):
        """ä»é¡µé¢æå–ç›¸å…³é“¾æ¥"""
        if not html:
            return []
        
        soup = BeautifulSoup(html, 'html.parser')
        links = []
        
        # å¤šç§é“¾æ¥é€‰æ‹©å™¨
        selectors = [
            'a[href*="detail"]',
            'a[href*="article"]',
            'a[href*="policy"]',
            'a[href*="talent"]',
            'a[href*="äººæ‰"]',
            '.list-item a',
            '.title a',
            '.policy-title a',
            '.news-title a',
            'li a',
            '.content a'
        ]
        
        found_links = []
        for selector in selectors:
            found_links.extend(soup.select(selector))
        
        # è¿‡æ»¤å’Œè¯†åˆ«äººæ‰ç›¸å…³é“¾æ¥
        seen_hrefs = set()
        for a in found_links:
            href = a.get('href')
            title = a.get_text(strip=True)
            
            if href and href not in seen_hrefs and len(title) > 3:
                # æ£€æŸ¥æ˜¯å¦ä¸äººæ‰ç›¸å…³
                is_talent_related = (
                    any(keyword in title for keyword in self.talent_keywords) or
                    any(keyword in title for keyword in self.company_keywords) or
                    'äººæ‰' in title or 'ä½æˆ¿' in title or 'è½æˆ·' in title or 
                    'åšå£«' in title or 'ç¡•å£«' in title or 'ä¸“å®¶' in title or
                    'å¼•è¿›' in title or 'è¡¥è´´' in title or 'å…¬å¯“' in title
                )
                
                if is_talent_related:
                    full_url = urljoin(base_url, href)
                    links.append({
                        'title': title,
                        'url': full_url
                    })
                    seen_hrefs.add(href)
        
        return links

    def extract_content_from_url(self, url):
        """ä»URLæå–å†…å®¹"""
        html = self.fetch_page(url)
        if not html:
            return None
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # æå–æ ‡é¢˜
        title = ""
        title_selectors = [
            'h1', 'h2', 'h3', '.title', '.art-title', '.page-title',
            '[class*="title"]', '[class*="head"]', '.main-title', '.news-title'
        ]
        for selector in title_selectors:
            elem = soup.select_one(selector)
            if elem:
                candidate_title = elem.get_text(strip=True)
                if len(candidate_title) > 5 and len(candidate_title) < 200:
                    title = candidate_title
                    break
        
        # æå–å†…å®¹
        content = ""
        content_selectors = [
            '.content', '.art-content', '.main-content', '.policy-content',
            '[class*="content"]', '.main', 'article', '.detail', '.text', '.body'
        ]
        for selector in content_selectors:
            elem = soup.select_one(selector)
            if elem:
                # ç§»é™¤å¯¼èˆªç­‰æ— å…³å…ƒç´ 
                for nav in elem.select('nav, .nav, .menu, .breadcrumb, .pagination, .sidebar'):
                    nav.decompose()
                content = elem.get_text(strip=True)
                if len(content) > 200:
                    break
        
        if not content or len(content) < 100:
            body = soup.find('body')
            if body:
                for elem in body.select('nav, .nav, .menu, .sidebar, .footer, .header, .breadcrumb'):
                    elem.decompose()
                content = body.get_text(strip=True)
        
        # æå–æ—¶é—´
        publish_date = ""
        date_patterns = [
            r'(\d{4}[-å¹´]\d{1,2}[-æœˆ]\d{1,2}[æ—¥]?)',
            r'(\d{4}/\d{1,2}/\d{1,2})',
            r'(\d{4}\.\d{1,2}\.\d{1,2})',
            r'å‘å¸ƒæ—¶é—´[ï¼š:]\s*(\d{4}[-å¹´]\d{1,2}[-æœˆ]\d{1,2})',
            r'æ—¶é—´[ï¼š:]\s*(\d{4}[-å¹´]\d{1,2}[-æœˆ]\d{1,2})'
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, content + title)
            if match:
                publish_date = match.group(1)
                break
        
        # æå–éƒ¨é—¨
        department = ""
        dept_patterns = [
            r'å‘å¸ƒæœºæ„[ï¼š:]\s*([^ï¼Œã€‚ï¼›ï¼ï¼Ÿ\n]+)',
            r'å‘å¸ƒéƒ¨é—¨[ï¼š:]\s*([^ï¼Œã€‚ï¼›ï¼ï¼Ÿ\n]+)',
            r'(å¾æ±‡åŒº[^ï¼Œã€‚ï¼›ï¼ï¼Ÿ\n]*?[å±€|å§”|åŠ|ä¸­å¿ƒ|éƒ¨|å¤„])',
            r'(ä¸Šæµ·å¸‚å¾æ±‡åŒº[^ï¼Œã€‚ï¼›ï¼ï¼Ÿ\n]*?[å±€|å§”|åŠ|ä¸­å¿ƒ|éƒ¨|å¤„])'
        ]
        
        for pattern in dept_patterns:
            match = re.search(pattern, content + title)
            if match:
                department = match.group(1).strip()
                break
        
        return {
            'title': title,
            'url': url,
            'content': content[:8000],  # å¢åŠ å†…å®¹é•¿åº¦
            'publish_date': publish_date,
            'department': department,
            'crawl_time': datetime.now().isoformat()
        }

    def classify_policy(self, policy):
        """æ”¿ç­–åˆ†ç±»"""
        text = f"{policy['title']} {policy['content']}"
        
        categories = {
            "äººæ‰å¼•è¿›": [
                "äººæ‰å¼•è¿›", "äººæ‰æ‹›è˜", "æµ·å¤–äººæ‰", "é«˜å±‚æ¬¡äººæ‰", "é¢†å†›äººæ‰",
                "ä¸“å®¶å¼•è¿›", "åšå£«å¼•è¿›", "ç¡•å£«å¼•è¿›", "äººæ‰è®¡åˆ’", "åƒäººè®¡åˆ’"
            ],
            "ä½æˆ¿ä¿éšœ": [
                "ä½æˆ¿è¡¥è´´", "æˆ¿å±‹è¡¥è´´", "ç§Ÿæˆ¿è¡¥è´´", "äººæ‰å…¬å¯“", "ä½æˆ¿æ”¯æŒ",
                "ä¿éšœæ€§ä½æˆ¿", "å…¬ç§Ÿæˆ¿", "å»‰ç§Ÿæˆ¿", "å®‰å±…", "ç§Ÿé‡‘", "ä½æˆ¿ä¼˜æƒ "
            ],
            "è½æˆ·æœåŠ¡": [
                "è½æˆ·", "æˆ·ç±", "å±…ä½è¯", "ç§¯åˆ†", "è¿æˆ·", "æˆ·å£", "è½æˆ·æ”¿ç­–"
            ],
            "å­å¥³æ•™è‚²": [
                "å­å¥³å…¥å­¦", "å­å¥³æ•™è‚²", "æ•™è‚²ä¼˜æƒ ", "å…¥å­¦", "å­¦åŒº", "æ‹©æ ¡"
            ],
            "åˆ›ä¸šæ‰¶æŒ": [
                "åˆ›ä¸šæ”¯æŒ", "åˆ›ä¸šè¡¥è´´", "åˆ›ä¸šå­µåŒ–", "åˆ›ä¸šåŸºé‡‘", "åˆåˆ›", "ä¼—åˆ›"
            ],
            "èµ„é‡‘èµ„åŠ©": [
                "äººæ‰èµ„åŠ©", "äººæ‰å¥–åŠ±", "ç§‘ç ”èµ„åŠ©", "é¡¹ç›®èµ„åŠ©", "æ´¥è´´", "è¡¥åŠ©", "å®‰å®¶è´¹"
            ],
            "åŒ»ç–—æœåŠ¡": [
                "åŒ»ç–—ä¿éšœ", "åŒ»ç–—æœåŠ¡", "å¥åº·", "ä½“æ£€", "åŒ»ç–—ä¼˜æƒ ", "å°±åŒ»"
            ]
        }
        
        # è®¡ç®—å¾—åˆ†
        category_scores = {}
        for category, keywords in categories.items():
            score = 0
            for keyword in keywords:
                if keyword in policy['title']:
                    score += 3
                if keyword in policy['content']:
                    score += 1
            category_scores[category] = score
        
        if category_scores:
            best_category = max(category_scores, key=category_scores.get)
            if category_scores[best_category] > 0:
                return best_category
        
        return "å…¶ä»–"

    def extract_company_requirements(self, policy):
        """æå–ä¼ä¸šç”³æŠ¥è¦æ±‚"""
        text = f"{policy['title']} {policy['content']}"
        
        # ä¼ä¸šè¦æ±‚å…³é”®è¯åˆ†ç±»
        requirement_patterns = {
            "ä¼ä¸šåŸºæœ¬æ¡ä»¶": [
                "æ³¨å†Œåœ°", "æ³¨å†Œèµ„æœ¬", "å®ç¼´èµ„æœ¬", "è¥ä¸šæ‰§ç…§", "ç»è¥æœŸé™",
                "ä¼ä¸šæ€§è´¨", "è¡Œä¸šç±»åˆ«", "ç»è¥èŒƒå›´", "çº³ç¨", "ä¿¡ç”¨è®°å½•",
                "é«˜æ–°æŠ€æœ¯ä¼ä¸š", "ä¸“ç²¾ç‰¹æ–°", "è§„æ¨¡ä»¥ä¸Šä¼ä¸š", "ä¸Šå¸‚å…¬å¸"
            ],
            "äººæ‰è¦æ±‚": [
                "å­¦å†è¦æ±‚", "å­¦ä½è¦æ±‚", "ä¸“ä¸šè¦æ±‚", "å·¥ä½œç»éªŒ", "å¹´é¾„é™åˆ¶",
                "æŠ€æœ¯èŒç§°", "ä¸“ä¸šæŠ€èƒ½", "æµ·å¤–ç»å†", "è·å¥–æƒ…å†µ", "å­¦æœ¯æˆæœ"
            ],
            "ç”³æŠ¥ææ–™": [
                "ç”³æŠ¥è¡¨", "ç”³è¯·è¡¨", "èº«ä»½è¯æ˜", "å­¦å†è¯æ˜", "å·¥ä½œè¯æ˜",
                "æ¨èä¿¡", "ç®€å†", "ä¸šç»©ææ–™", "è·å¥–è¯ä¹¦", "ä¸“åˆ©è¯ä¹¦"
            ],
            "ç”³æŠ¥æ—¶é—´": [
                "ç”³æŠ¥æ—¶é—´", "æˆªæ­¢æ—¶é—´", "å—ç†æœŸé—´", "è¯„å®¡æ—¶é—´", "å…¬ç¤ºæ—¶é—´",
                "åŠç†æ—¶é™", "æœ‰æ•ˆæœŸ", "å¹´åº¦ç”³æŠ¥", "å¸¸å¹´å—ç†"
            ],
            "æ”¯æŒæ ‡å‡†": [
                "è¡¥è´´æ ‡å‡†", "å¥–åŠ±é‡‘é¢", "æ”¯æŒé¢åº¦", "æœ€é«˜é™é¢", "ä¸€æ¬¡æ€§",
                "æŒ‰å¹´åº¦", "åˆ†æœŸæ‹¨ä»˜", "é…å¥—èµ„é‡‘", "ä¸“é¡¹èµ„é‡‘"
            ]
        }
        
        requirements = {}
        for req_type, keywords in requirement_patterns.items():
            found_items = []
            
            # æŒ‰å¥å­åˆ†å‰²å¹¶æœç´¢
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›\n]', text)
            for sentence in sentences:
                sentence = sentence.strip()
                if len(sentence) > 15 and len(sentence) < 500:
                    for keyword in keywords:
                        if keyword in sentence:
                            # è¿‡æ»¤æ— æ•ˆå¥å­
                            if not sentence.startswith(('é¦–é¡µ', 'è¿”å›', 'ä¸Šä¸€é¡µ', 'ä¸‹ä¸€é¡µ', 'ç‚¹å‡»')):
                                found_items.append(sentence)
                                break
            
            # å»é‡å¹¶é™åˆ¶æ•°é‡
            unique_items = []
            seen = set()
            for item in found_items:
                if item not in seen:
                    unique_items.append(item)
                    seen.add(item)
                if len(unique_items) >= 5:
                    break
            
            requirements[req_type] = "; ".join(unique_items) if unique_items else ""
        
        return requirements

    def crawl_policies(self):
        """çˆ¬å–æ”¿ç­–"""
        print("å¼€å§‹å…¨é¢çˆ¬å–å¾æ±‡åŒºä¼ä¸šäººæ‰æ”¿ç­–...")
        print(f"å°†çˆ¬å– {len(self.urls)} ä¸ªæ”¿ç­–æº")
        
        os.makedirs('data', exist_ok=True)
        
        all_links = []
        
        # ç¬¬ä¸€æ­¥ï¼šä»å„ä¸ªé¡µé¢æ”¶é›†é“¾æ¥
        for url in self.urls:
            html = self.fetch_page(url)
            if html:
                links = self.extract_links_from_page(html, url)
                all_links.extend(links)
                if links:
                    print(f"ä» {url} è·å–åˆ° {len(links)} ä¸ªç›¸å…³é“¾æ¥")
            time.sleep(1)
        
        # ç¬¬äºŒæ­¥ï¼šå»é‡
        unique_links = []
        seen_urls = set()
        for link in all_links:
            if link['url'] not in seen_urls:
                unique_links.append(link)
                seen_urls.add(link['url'])
        
        print(f"å»é‡åå…±æ‰¾åˆ° {len(unique_links)} ä¸ªäººæ‰ç›¸å…³é“¾æ¥")
        
        # ç¬¬ä¸‰æ­¥ï¼šæå–è¯¦ç»†å†…å®¹ï¼ˆå¢åŠ åˆ°80ä¸ªï¼‰
        target_links = unique_links[:80]
        print(f"å°†è¯¦ç»†çˆ¬å–å‰ {len(target_links)} ä¸ªæ”¿ç­–")
        
        for i, link in enumerate(target_links, 1):
            print(f"æ­£åœ¨å¤„ç†ç¬¬ {i}/{len(target_links)} ä¸ª: {link['title'][:50]}...")
            
            policy = self.extract_content_from_url(link['url'])
            if policy and len(policy['content']) > 200:
                policy['category'] = self.classify_policy(policy)
                policy['company_requirements'] = self.extract_company_requirements(policy)
                
                # è¿‡æ»¤ï¼šåªä¿ç•™æœ‰å®é™…å†…å®¹çš„æ”¿ç­–
                has_meaningful_content = (
                    len(policy['content']) > 500 or
                    any(len(v.strip()) > 20 for v in policy['company_requirements'].values() if v) or
                    any(keyword in policy['title'] + policy['content'] for keyword in 
                        ['äººæ‰', 'ä½æˆ¿', 'è½æˆ·', 'åšå£«', 'ç¡•å£«', 'è¡¥è´´', 'å…¬å¯“', 'å¼•è¿›'])
                )
                
                if has_meaningful_content:
                    self.policies.append(policy)
                    print(f"  âœ… å·²ä¿å­˜ (åˆ†ç±»: {policy['category']})")
                else:
                    print(f"  âŒ è·³è¿‡ (å†…å®¹ä¸è¶³)")
            
            time.sleep(1)
        
        print(f"\nğŸ¯ æˆåŠŸçˆ¬å– {len(self.policies)} ä¸ªäººæ‰æ”¿ç­–")

    def export_results(self):
        """å¯¼å‡ºç»“æœ"""
        if not self.policies:
            print("æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")
            return
        
        print("æ­£åœ¨å¯¼å‡ºæ•°æ®...")
        
        # ç»Ÿè®¡
        categories = {}
        departments = {}
        for policy in self.policies:
            cat = policy.get('category', 'å…¶ä»–')
            categories[cat] = categories.get(cat, 0) + 1
            
            dept = policy.get('department', 'æœªçŸ¥')
            departments[dept] = departments.get(dept, 0) + 1
        
        # JSONå¯¼å‡º
        output_data = {
            'crawl_time': datetime.now().isoformat(),
            'total_policies': len(self.policies),
            'categories': categories,
            'departments': departments,
            'policies': self.policies
        }
        
        with open('data/comprehensive_talent_policies.json', 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        # Excelå¯¼å‡º
        df_data = []
        for policy in self.policies:
            req = policy.get('company_requirements', {})
            df_data.append({
                'æ ‡é¢˜': policy.get('title', ''),
                'åˆ†ç±»': policy.get('category', ''),
                'å‘å¸ƒéƒ¨é—¨': policy.get('department', ''),
                'å‘å¸ƒæ—¶é—´': policy.get('publish_date', ''),
                'é“¾æ¥': policy.get('url', ''),
                'ä¼ä¸šåŸºæœ¬æ¡ä»¶': req.get('ä¼ä¸šåŸºæœ¬æ¡ä»¶', ''),
                'äººæ‰è¦æ±‚': req.get('äººæ‰è¦æ±‚', ''),
                'ç”³æŠ¥ææ–™': req.get('ç”³æŠ¥ææ–™', ''),
                'ç”³æŠ¥æ—¶é—´': req.get('ç”³æŠ¥æ—¶é—´', ''),
                'æ”¯æŒæ ‡å‡†': req.get('æ”¯æŒæ ‡å‡†', ''),
                'å†…å®¹æ‘˜è¦': policy.get('content', '')[:500]
            })
        
        df = pd.DataFrame(df_data)
        df.to_csv('data/comprehensive_talent_policies.csv', index=False, encoding='utf-8-sig')
        df.to_excel('data/comprehensive_talent_policies.xlsx', index=False)
        
        # è¯¦ç»†æŠ¥å‘Š
        with open('data/comprehensive_talent_report.txt', 'w', encoding='utf-8') as f:
            f.write("å¾æ±‡åŒºä¼ä¸šäººæ‰æ”¿ç­–ç»¼åˆæŠ¥å‘Š\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"çˆ¬å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ€»æ”¿ç­–æ•°é‡: {len(self.policies)} æ¡\n\n")
            
            f.write("ğŸ“Š æ”¿ç­–åˆ†ç±»åˆ†å¸ƒ:\n")
            for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
                percentage = count / len(self.policies) * 100
                f.write(f"   {cat}: {count}æ¡ ({percentage:.1f}%)\n")
            
            f.write(f"\nğŸ¢ ä¸»è¦å‘å¸ƒéƒ¨é—¨:\n")
            for dept, count in sorted(departments.items(), key=lambda x: x[1], reverse=True)[:10]:
                if count > 0 and dept != 'æœªçŸ¥':
                    f.write(f"   {dept}: {count}æ¡\n")
            
            f.write(f"\n\nğŸ“‹ è¯¦ç»†æ”¿ç­–åˆ—è¡¨:\n")
            f.write("-" * 80 + "\n")
            
            for i, policy in enumerate(self.policies, 1):
                f.write(f"\n{i}. {policy.get('title', '')}\n")
                f.write(f"   åˆ†ç±»: {policy.get('category', '')}\n")
                f.write(f"   éƒ¨é—¨: {policy.get('department', '')}\n")
                f.write(f"   æ—¶é—´: {policy.get('publish_date', '')}\n")
                f.write(f"   é“¾æ¥: {policy.get('url', '')}\n")
                
                req = policy.get('company_requirements', {})
                for req_type, req_content in req.items():
                    if req_content.strip():
                        f.write(f"   ã€{req_type}ã€‘: {req_content[:200]}...\n")
                
                f.write("-" * 80 + "\n")
        
        print(f"\nğŸ“ æ•°æ®å·²å¯¼å‡º:")
        print(f"   ğŸ“‹ å®Œæ•´æ•°æ®: data/comprehensive_talent_policies.json")
        print(f"   ğŸ“Š è¡¨æ ¼æ•°æ®: data/comprehensive_talent_policies.csv")
        print(f"   ğŸ“Š Excelæ–‡ä»¶: data/comprehensive_talent_policies.xlsx")
        print(f"   ğŸ“„ è¯¦ç»†æŠ¥å‘Š: data/comprehensive_talent_report.txt")

def main():
    crawler = ComprehensiveTalentCrawler()
    crawler.crawl_policies()
    crawler.export_results()
    
    print("\nğŸ¯ ç»¼åˆäººæ‰æ”¿ç­–çˆ¬å–å®Œæˆï¼")

if __name__ == "__main__":
    main() 